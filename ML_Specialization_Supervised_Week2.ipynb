{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Specialization - Supervised Learning: Regression and Classification\n",
    "## Week 2: Multiple Linear Regression and Feature Engineering\n",
    "\n",
    "### Learning Objectives:\n",
    "- Extend linear regression to multiple features\n",
    "- Understand feature engineering and polynomial regression\n",
    "- Learn about learning curves and debugging algorithms\n",
    "- Apply normalization and feature scaling\n",
    "\n",
    "### Key Concepts:\n",
    "- **Multiple Linear Regression**: Predicting with multiple input features\n",
    "- **Feature Engineering**: Creating new features to improve model performance\n",
    "- **Polynomial Regression**: Using polynomial features for non-linear relationships\n",
    "- **Feature Scaling**: Normalizing features to improve gradient descent convergence\n",
    "- **Learning Curves**: Diagnosing bias and variance in models\n",
    "\n",
    "Building on last week's linear regression with one variable, we'll now work with multiple features and learn techniques to improve our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Required Libraries\n",
    "\n",
    "Let's import the necessary libraries for our exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression, load_boston\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate Multiple Feature Dataset\n",
    "\n",
    "We'll create a dataset with multiple features to predict house prices based on size, number of bedrooms, and age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for house prices with multiple features\n",
    "X, y = make_regression(n_samples=200, n_features=3, noise=15, random_state=42)\n",
    "\n",
    "# Add intercept term (x0 = 1) to X\n",
    "X = np.column_stack([np.ones(X.shape[0]), X])\n",
    "\n",
    "# Reshape y to be a column vector\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Feature names for interpretability\n",
    "feature_names = ['Intercept', 'Size', 'Bedrooms', 'Age']\n",
    "\n",
    "print(f\"Dataset shape: X = {X.shape}, y = {y.shape}\")\n",
    "print(f\"Features: {feature_names}\")\n",
    "print(f\"First 5 samples:\")\n",
    "for i in range(5):\n",
    "    print(f\"Size: {X[i, 1]:.2f}, Bedrooms: {X[i, 2]:.2f}, Age: {X[i, 3]:.2f}, Price: {y[i, 0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Scaling (Normalization)\n",
    "\n",
    "Feature scaling is crucial for gradient descent convergence when features have different scales. We'll implement z-score normalization:\n",
    "\n",
    "$$x_j^{(i)} := \\frac{x_j^{(i)} - \\mu_j}{ \\sigma_j }$$\n",
    "\n",
    "Where:\n",
    "- $\\mu_j$ is the mean of feature j\n",
    "- $\\sigma_j$ is the standard deviation of feature j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_normalize(X):\n",
    "    \"\"\"\n",
    "    Normalize features using z-score normalization.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix (m x n), excluding intercept term\n",
    "    \n",
    "    Returns:\n",
    "        X_norm: Normalized feature matrix\n",
    "        mu: Mean of each feature\n",
    "        sigma: Standard deviation of each feature\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE - Implement feature normalization\n",
    "    # Hint: Don't normalize the intercept term (first column)\n",
    "    mu = None  # Replace with your implementation\n",
    "    sigma = None  # Replace with your implementation\n",
    "    X_norm = None  # Replace with your implementation\n",
    "    \n",
    "    return X_norm, mu, sigma\n",
    "\n",
    "# Normalize features (excluding intercept)\n",
    "X_norm, mu, sigma = feature_normalize(X[:, 1:])\n",
    "\n",
    "# Add intercept back\n",
    "X_norm = np.column_stack([np.ones(X_norm.shape[0]), X_norm])\n",
    "\n",
    "print(f\"Original X range: [{X.min():.2f}, {X.max():.2f}]\")\n",
    "print(f\"Normalized X range: [{X_norm.min():.2f}, {X_norm.max():.2f}]\")\n",
    "print(f\"Feature means: {mu}\")\n",
    "print(f\"Feature std devs: {sigma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Multiple Linear Regression Implementation\n",
    "\n",
    "Now we'll implement linear regression for multiple features. The hypothesis remains the same:\n",
    "\n",
    "$$h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n$$\n",
    "\n",
    "But now we have multiple features to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, theta):\n",
    "    \"\"\"\n",
    "    Make predictions using linear regression model.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix (m x n)\n",
    "        theta: Parameter vector (n x 1)\n",
    "    \n",
    "    Returns:\n",
    "        predictions: Predicted values (m x 1)\n",
    "    \"\"\"\n",
    "    return np.dot(X, theta)\n",
    "\n",
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the cost function for linear regression.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix (m x n)\n",
    "        y: Target values (m x 1)\n",
    "        theta: Parameter vector (n x 1)\n",
    "    \n",
    "    Returns:\n",
    "        cost: The cost value (scalar)\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    predictions = predict(X, theta)\n",
    "    errors = predictions - y\n",
    "    cost = (1 / (2 * m)) * np.sum(errors ** 2)\n",
    "    return cost\n",
    "\n",
    "def gradient_descent(X, y, theta, alpha, num_iterations):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to learn theta.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix (m x n)\n",
    "        y: Target values (m x 1)\n",
    "        theta: Initial parameter vector (n x 1)\n",
    "        alpha: Learning rate\n",
    "        num_iterations: Number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        theta: Optimized parameter vector\n",
    "        cost_history: List of cost values over iterations\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        predictions = predict(X, theta)\n",
    "        errors = predictions - y\n",
    "        gradient = (1 / m) * np.dot(X.T, errors)\n",
    "        theta = theta - alpha * gradient\n",
    "        \n",
    "        cost = compute_cost(X, y, theta)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"Iteration {iteration}: Cost = {cost:.4f}\")\n",
    "    \n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training the Model\n",
    "\n",
    "Let's train our multiple linear regression model using the normalized features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize theta with zeros\n",
    "theta_initial = np.zeros((X_norm.shape[1], 1))\n",
    "\n",
    "# Set hyperparameters\n",
    "alpha = 0.1\n",
    "num_iterations = 1000\n",
    "\n",
    "# Run gradient descent\n",
    "theta_optimized, cost_history = gradient_descent(X_norm, y, theta_initial, alpha, num_iterations)\n",
    "\n",
    "print(f\"\\nOptimized theta: {theta_optimized.flatten()}\")\n",
    "print(f\"Final cost: {cost_history[-1]:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = predict(X_norm, theta_optimized)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Feature Engineering - Polynomial Regression\n",
    "\n",
    "Sometimes linear relationships aren't sufficient. We can create polynomial features to capture non-linear relationships:\n",
    "\n",
    "For example, if we have feature x, we can create: $x^2$, $x^3$, etc.\n",
    "\n",
    "This allows us to fit curves instead of straight lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create polynomial features\n",
    "def create_polynomial_features(X, degree):\n",
    "    \"\"\"\n",
    "    Create polynomial features up to the specified degree.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix (excluding intercept)\n",
    "        degree: Maximum polynomial degree\n",
    "    \n",
    "    Returns:\n",
    "        X_poly: Polynomial feature matrix\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE - Implement polynomial feature creation\n",
    "    # Hint: Use loops or vectorized operations to create x^2, x^3, etc.\n",
    "    X_poly = None  # Replace with your implementation\n",
    "    \n",
    "    return X_poly\n",
    "\n",
    "# Create polynomial features (degree 2)\n",
    "X_poly = create_polynomial_features(X[:, 1:], 2)  # Exclude intercept\n",
    "X_poly = np.column_stack([np.ones(X_poly.shape[0]), X_poly])\n",
    "\n",
    "print(f\"Original features: {X.shape[1]}\")\n",
    "print(f\"Polynomial features (degree 2): {X_poly.shape[1]}\")\n",
    "\n",
    "# Normalize polynomial features\n",
    "X_poly_norm, _, _ = feature_normalize(X_poly[:, 1:])\n",
    "X_poly_norm = np.column_stack([np.ones(X_poly_norm.shape[0]), X_poly_norm])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Training Polynomial Regression\n",
    "\n",
    "Let's train a polynomial regression model and compare it with linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train polynomial regression\n",
    "theta_poly = np.zeros((X_poly_norm.shape[1], 1))\n",
    "theta_poly_opt, cost_poly_history = gradient_descent(X_poly_norm, y, theta_poly, alpha, num_iterations)\n",
    "\n",
    "print(f\"Polynomial theta: {theta_poly_opt.flatten()}\")\n",
    "print(f\"Polynomial final cost: {cost_poly_history[-1]:.4f}\")\n",
    "\n",
    "# Compare predictions\n",
    "y_pred_linear = predict(X_norm, theta_optimized)\n",
    "y_pred_poly = predict(X_poly_norm, theta_poly_opt)\n",
    "\n",
    "mse_linear = mean_squared_error(y, y_pred_linear)\n",
    "mse_poly = mean_squared_error(y, y_pred_poly)\n",
    "\n",
    "print(f\"\\nLinear Regression MSE: {mse_linear:.4f}\")\n",
    "print(f\"Polynomial Regression MSE: {mse_poly:.4f}\")\n",
    "print(f\"Improvement: {((mse_linear - mse_poly) / mse_linear * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Learning Curves\n",
    "\n",
    "Learning curves help us diagnose bias and variance problems:\n",
    "\n",
    "- **High bias (underfitting)**: Both training and validation error are high\n",
    "- **High variance (overfitting)**: Training error is low, but validation error is high\n",
    "\n",
    "Let's implement learning curves by training on different dataset sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_curve(X, y, X_val, y_val, alpha, num_iterations):\n",
    "    \"\"\"\n",
    "    Generate learning curve data.\n",
    "    \n",
    "    Args:\n",
    "        X: Training feature matrix\n",
    "        y: Training target values\n",
    "        X_val: Validation feature matrix\n",
    "        y_val: Validation target values\n",
    "        alpha: Learning rate\n",
    "        num_iterations: Number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        train_errors: Training errors for different dataset sizes\n",
    "        val_errors: Validation errors for different dataset sizes\n",
    "        m_values: Dataset sizes used\n",
    "    \"\"\"\n",
    "    m_values = np.arange(1, len(X) + 1, 10)  # Every 10th sample\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "    \n",
    "    # YOUR CODE HERE - Implement learning curve generation\n",
    "    # Hint: Train on progressively larger subsets and compute errors\n",
    "    \n",
    "    return train_errors, val_errors, m_values\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_norm, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Generate learning curves for linear regression\n",
    "train_errors, val_errors, m_values = learning_curve(X_train, y_train, X_val, y_val, alpha, num_iterations)\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(m_values, train_errors, 'b-', label='Training Error')\n",
    "plt.plot(m_values, val_errors, 'r-', label='Validation Error')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Learning Curves - Linear Regression')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Model Comparison and Analysis\n",
    "\n",
    "Let's compare our linear and polynomial models and analyze their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cost histories\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(cost_history, label='Linear Regression')\n",
    "plt.plot(cost_poly_history, label='Polynomial Regression')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost vs Iterations')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot predictions vs actual\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y, y_pred_linear, alpha=0.7, label='Linear', color='blue')\n",
    "plt.scatter(y, y_pred_poly, alpha=0.7, label='Polynomial', color='red')\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', linewidth=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Predictions vs Actual')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print model coefficients interpretation\n",
    "print(\"Linear Regression Coefficients:\")\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(f\"{name}: {theta_optimized[i, 0]:.4f}\")\n",
    "\n",
    "print(\"\\nPolynomial Regression has\", X_poly_norm.shape[1], \"coefficients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Experimentation and Questions\n",
    "\n",
    "Now let's explore some important concepts:\n",
    "\n",
    "1. **Feature Scaling Impact**: Compare gradient descent convergence with and without feature scaling.\n",
    "\n",
    "2. **Polynomial Degree**: Try different polynomial degrees (1, 2, 3, 4) and observe the effects on training and validation error.\n",
    "\n",
    "3. **Learning Rate Tuning**: Experiment with different learning rates and see how they affect convergence.\n",
    "\n",
    "4. **Bias-Variance Tradeoff**: Based on your learning curves, does your model suffer from high bias or high variance?\n",
    "\n",
    "5. **Feature Selection**: Which features seem most important? How could you determine this systematically?\n",
    "\n",
    "**Challenge**: Implement k-fold cross-validation to get a more robust estimate of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Different polynomial degrees\n",
    "degrees = [1, 2, 3, 4]\n",
    "mse_train_list = []\n",
    "mse_val_list = []\n",
    "\n",
    "for degree in degrees:\n",
    "    # Create polynomial features\n",
    "    X_poly_exp = create_polynomial_features(X_train[:, 1:], degree)\n",
    "    X_poly_exp = np.column_stack([np.ones(X_poly_exp.shape[0]), X_poly_exp])\n",
    "    \n",
    "    # Normalize\n",
    "    X_poly_exp_norm, _, _ = feature_normalize(X_poly_exp[:, 1:])\n",
    "    X_poly_exp_norm = np.column_stack([np.ones(X_poly_exp_norm.shape[0]), X_poly_exp_norm])\n",
    "    \n",
    "    # Train model\n",
    "    theta_exp = np.zeros((X_poly_exp_norm.shape[1], 1))\n",
    "    theta_exp_opt, _ = gradient_descent(X_poly_exp_norm, y_train, theta_exp, alpha, num_iterations)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_train_pred = predict(X_poly_exp_norm, theta_exp_opt)\n",
    "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "    mse_train_list.append(mse_train)\n",
    "    \n",
    "    # For validation, we need to create the same polynomial features\n",
    "    X_val_poly = create_polynomial_features(X_val[:, 1:], degree)\n",
    "    X_val_poly = np.column_stack([np.ones(X_val_poly.shape[0]), X_val_poly])\n",
    "    X_val_poly_norm, _, _ = feature_normalize(X_val_poly[:, 1:])\n",
    "    X_val_poly_norm = np.column_stack([np.ones(X_val_poly_norm.shape[0]), X_val_poly_norm])\n",
    "    \n",
    "    y_val_pred = predict(X_val_poly_norm, theta_exp_opt)\n",
    "    mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "    mse_val_list.append(mse_val)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(degrees, mse_train_list, 'b-o', label='Training Error')\n",
    "plt.plot(degrees, mse_val_list, 'r-o', label='Validation Error')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Bias-Variance Tradeoff: Polynomial Degree vs Error')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Training MSE by degree:\", [f\"{mse:.2f}\" for mse in mse_train_list])\n",
    "print(\"Validation MSE by degree:\", [f\"{mse:.2f}\" for mse in mse_val_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "1. **Multiple Linear Regression** extends single-variable regression to handle multiple features simultaneously.\n",
    "\n",
    "2. **Feature Scaling** is crucial for gradient descent convergence when features have different scales.\n",
    "\n",
    "3. **Feature Engineering** (like polynomial features) allows us to capture non-linear relationships.\n",
    "\n",
    "4. **Learning Curves** help diagnose whether a model has high bias (underfitting) or high variance (overfitting).\n",
    "\n",
    "5. **Bias-Variance Tradeoff**: More complex models (higher polynomial degrees) can reduce bias but increase variance.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll move from regression to classification problems and learn about logistic regression for binary classification tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
