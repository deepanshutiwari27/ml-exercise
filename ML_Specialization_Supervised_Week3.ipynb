{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Specialization - Supervised Learning: Regression and Classification\n",
    "## Week 3: Logistic Regression and Regularization\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand logistic regression for binary classification\n",
    "- Implement the sigmoid function and logistic cost function\n",
    "- Apply regularization techniques to prevent overfitting\n",
    "- Compare regularized vs non-regularized models\n",
    "\n",
    "### Key Concepts:\n",
    "- **Logistic Regression**: A classification algorithm that predicts probabilities\n",
    "- **Sigmoid Function**: Maps any real value to a probability between 0 and 1\n",
    "- **Logistic Cost Function**: Measures error for classification tasks\n",
    "- **Regularization**: Techniques to prevent overfitting (L1, L2)\n",
    "- **Overfitting Prevention**: Methods to improve model generalization\n",
    "\n",
    "This week we transition from regression to classification, learning how to predict discrete categories instead of continuous values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Required Libraries\n",
    "\n",
    "Let's import the necessary libraries for our classification exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate Binary Classification Dataset\n",
    "\n",
    "We'll create a dataset for binary classification. Let's use a medical diagnosis scenario where we predict whether a patient has a disease based on test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic binary classification data\n",
    "X, y = make_classification(n_samples=300, n_features=2, n_informative=2, n_redundant=0, \n",
    "                          n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "# Add intercept term (x0 = 1) to X\n",
    "X = np.column_stack([np.ones(X.shape[0]), X])\n",
    "\n",
    "# Ensure y is a column vector\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "print(f\"Dataset shape: X = {X.shape}, y = {y.shape}\")\n",
    "print(f\"Classes: {np.unique(y.flatten())}\")\n",
    "print(f\"Class distribution: Class 0 = {np.sum(y == 0)}, Class 1 = {np.sum(y == 1)}\")\n",
    "print(f\"First 5 samples:\")\n",
    "for i in range(5):\n",
    "    print(f\"Features: [{X[i, 1]:.2f}, {X[i, 2]:.2f}], Label: {y[i, 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualize the Classification Data\n",
    "\n",
    "Let's plot our data to understand the classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[y.flatten() == 0, 1], X[y.flatten() == 0, 2], color='blue', alpha=0.7, label='Class 0')\n",
    "plt.scatter(X[y.flatten() == 1, 1], X[y.flatten() == 1, 2], color='red', alpha=0.7, label='Class 1')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Binary Classification Dataset')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The Sigmoid Function\n",
    "\n",
    "Logistic regression uses the sigmoid function to map any real-valued number to a probability between 0 and 1:\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Where z is our linear combination of features: $z = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n$\n",
    "\n",
    "The hypothesis becomes: $h_\\theta(x) = \\sigma(z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute sigmoid function.\n",
    "    \n",
    "    Args:\n",
    "        z: Input value(s) (scalar, vector, or matrix)\n",
    "    \n",
    "    Returns:\n",
    "        sigmoid_value: Sigmoid of input (same shape as z)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE - Implement the sigmoid function\n",
    "    # Hint: Use np.exp() and be careful with numerical stability\n",
    "    sigmoid_value = None  # Replace with your implementation\n",
    "    \n",
    "    return sigmoid_value\n",
    "\n",
    "# Test the sigmoid function\n",
    "z_test = np.array([-5, -1, 0, 1, 5])\n",
    "sigmoid_test = sigmoid(z_test)\n",
    "print(f\"Sigmoid values for z = {z_test}: {sigmoid_test}\")\n",
    "\n",
    "# Plot sigmoid function\n",
    "z_range = np.linspace(-10, 10, 100)\n",
    "sigmoid_range = sigmoid(z_range)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(z_range, sigmoid_range, 'b-', linewidth=2)\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.7, label='Decision Boundary (0.5)')\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.7)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('σ(z)')\n",
    "plt.title('Sigmoid Function')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Logistic Regression Hypothesis\n",
    "\n",
    "Now we'll implement the logistic regression hypothesis function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(X, theta):\n",
    "    \"\"\"\n",
    "    Compute probability predictions using logistic regression.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix (m x n)\n",
    "        theta: Parameter vector (n x 1)\n",
    "    \n",
    "    Returns:\n",
    "        probabilities: Probability predictions (m x 1)\n",
    "    \"\"\"\n",
    "    z = np.dot(X, theta)\n",
    "    return sigmoid(z)\n",
    "\n",
    "def predict(X, theta, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Make binary predictions using logistic regression.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix (m x n)\n",
    "        theta: Parameter vector (n x 1)\n",
    "        threshold: Classification threshold\n",
    "    \n",
    "    Returns:\n",
    "        predictions: Binary predictions (m x 1)\n",
    "    \"\"\"\n",
    "    probabilities = predict_proba(X, theta)\n",
    "    return (probabilities >= threshold).astype(int)\n",
    "\n",
    "# Test the prediction functions\n",
    "theta_test = np.array([[0.5], [1.0], [-0.5]])\n",
    "proba_test = predict_proba(X[:5], theta_test)\n",
    "pred_test = predict(X[:5], theta_test)\n",
    "\n",
    "print(f\"Test probabilities: {proba_test.flatten()}\")\n",
    "print(f\"Test predictions: {pred_test.flatten()}\")\n",
    "print(f\"Actual labels: {y[:5].flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Logistic Regression Cost Function\n",
    "\n",
    "For logistic regression, we use the logistic loss (cross-entropy loss):\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m [y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)}))]$$\n",
    "\n",
    "This cost function penalizes wrong predictions more heavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the logistic regression cost function.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix (m x n)\n",
    "        y: Target values (m x 1)\n",
    "        theta: Parameter vector (n x 1)\n",
    "    \n",
    "    Returns:\n",
    "        cost: The cost value (scalar)\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    \n",
    "    # YOUR CODE HERE - Implement the logistic cost function\n",
    "    # Hint: Use predict_proba and be careful with log(0) issues\n",
    "    probabilities = None  # Replace with your implementation\n",
    "    cost = None  # Replace with your implementation\n",
    "    \n",
    "    return cost\n",
    "\n",
    "# Test the cost function\n",
    "theta_test = np.array([[0.0], [0.0], [0.0]])\n",
    "cost_test = compute_cost(X, y, theta_test)\n",
    "print(f\"Cost with zero theta: {cost_test:.4f}\")\n",
    "\n",
    "# Test with random theta\n",
    "theta_random = np.random.randn(3, 1) * 0.1\n",
    "cost_random = compute_cost(X, y, theta_random)\n",
    "print(f\"Cost with random theta: {cost_random:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Gradient Descent for Logistic Regression\n",
    "\n",
    "The gradient for logistic regression is:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$$\n",
    "\n",
    "Interestingly, this looks similar to linear regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, num_iterations):\n",
    "    \"\"\"\n",
    "    Perform gradient descent for logistic regression.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix (m x n)\n",
    "        y: Target values (m x 1)\n",
    "        theta: Initial parameter vector (n x 1)\n",
    "        alpha: Learning rate\n",
    "        num_iterations: Number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        theta: Optimized parameter vector\n",
    "        cost_history: List of cost values over iterations\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        # YOUR CODE HERE - Implement gradient descent for logistic regression\n",
    "        # Hint: The gradient computation is similar to linear regression\n",
    "        probabilities = None  # Replace with your implementation\n",
    "        errors = None  # Replace with your implementation\n",
    "        gradient = None  # Replace with your implementation\n",
    "        theta = None  # Replace with your implementation\n",
    "        \n",
    "        cost = compute_cost(X, y, theta)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"Iteration {iteration}: Cost = {cost:.4f}\")\n",
    "    \n",
    "    return theta, cost_history\n",
    "\n",
    "# Initialize theta with zeros\n",
    "theta_initial = np.zeros((X.shape[1], 1))\n",
    "\n",
    "# Train the model\n",
    "alpha = 0.1\n",
    "num_iterations = 1000\n",
    "theta_optimized, cost_history = gradient_descent(X, y, theta_initial, alpha, num_iterations)\n",
    "\n",
    "print(f\"\\nOptimized theta: {theta_optimized.flatten()}\")\n",
    "print(f\"Final cost: {cost_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Model Evaluation\n",
    "\n",
    "Let's evaluate our logistic regression model using various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_proba = predict_proba(X, theta_optimized)\n",
    "y_pred = predict(X, theta_optimized)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "            yticklabels=['Actual 0', 'Actual 1'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Decision Boundary Visualization\n",
    "\n",
    "Let's visualize the decision boundary learned by our logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y, theta):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary for logistic regression.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Target values\n",
    "        theta: Parameter vector\n",
    "    \"\"\"\n",
    "    # Plot data points\n",
    "    plt.scatter(X[y.flatten() == 0, 1], X[y.flatten() == 0, 2], color='blue', alpha=0.7, label='Class 0')\n",
    "    plt.scatter(X[y.flatten() == 1, 1], X[y.flatten() == 1, 2], color='red', alpha=0.7, label='Class 1')\n",
    "    \n",
    "    # Create grid for decision boundary\n",
    "    x1_min, x1_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    x2_min, x2_max = X[:, 2].min() - 1, X[:, 2].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 100),\n",
    "                          np.linspace(x2_min, x2_max, 100))\n",
    "    \n",
    "    # Create feature matrix for grid points\n",
    "    grid_points = np.column_stack([np.ones(xx1.ravel().shape[0]), xx1.ravel(), xx2.ravel()])\n",
    "    \n",
    "    # Predict probabilities for grid points\n",
    "    Z = predict_proba(grid_points, theta)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    \n",
    "    # Plot decision boundary (where probability = 0.5)\n",
    "    plt.contour(xx1, xx2, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "    \n",
    "    # Plot probability contours\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap='RdYlBu_r')\n",
    "    \n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title('Logistic Regression Decision Boundary')\n",
    "    plt.legend()\n",
    "    plt.colorbar(label='Probability of Class 1')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_decision_boundary(X, y, theta_optimized)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Regularization\n",
    "\n",
    "Regularization helps prevent overfitting by adding a penalty term to the cost function:\n",
    "\n",
    "- **L2 Regularization (Ridge)**: $J(\\theta) = J_{original} + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2$\n",
    "- **L1 Regularization (Lasso)**: $J(\\theta) = J_{original} + \\frac{\\lambda}{2m} \\sum_{j=1}^n |\\theta_j|$\n",
    "\n",
    "Where λ is the regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_regularized(X, y, theta, lambda_reg):\n",
    "    \"\"\"\n",
    "    Compute regularized logistic regression cost function.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix (m x n)\n",
    "        y: Target values (m x 1)\n",
    "        theta: Parameter vector (n x 1)\n",
    "        lambda_reg: Regularization parameter\n",
    "    \n",
    "    Returns:\n",
    "        cost: Regularized cost value\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    \n",
    "    # Original cost\n",
    "    probabilities = predict_proba(X, theta)\n",
    "    original_cost = -np.mean(y * np.log(probabilities + 1e-15) + (1 - y) * np.log(1 - probabilities + 1e-15))\n",
    "    \n",
    "    # Regularization term (L2)\n",
    "    reg_term = (lambda_reg / (2 * m)) * np.sum(theta[1:] ** 2)  # Exclude theta_0\n",
    "    \n",
    "    return original_cost + reg_term\n",
    "\n",
    "def gradient_descent_regularized(X, y, theta, alpha, num_iterations, lambda_reg):\n",
    "    \"\"\"\n",
    "    Perform regularized gradient descent for logistic regression.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix (m x n)\n",
    "        y: Target values (m x 1)\n",
    "        theta: Initial parameter vector (n x 1)\n",
    "        alpha: Learning rate\n",
    "        num_iterations: Number of iterations\n",
    "        lambda_reg: Regularization parameter\n",
    "    \n",
    "    Returns:\n",
    "        theta: Optimized parameter vector\n",
    "        cost_history: List of cost values over iterations\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        probabilities = predict_proba(X, theta)\n",
    "        errors = probabilities - y\n",
    "        \n",
    "        # Regularized gradient\n",
    "        gradient = (1 / m) * np.dot(X.T, errors)\n",
    "        gradient[1:] += (lambda_reg / m) * theta[1:]  # Regularization for theta_1 to theta_n\n",
    "        \n",
    "        theta = theta - alpha * gradient\n",
    "        \n",
    "        cost = compute_cost_regularized(X, y, theta, lambda_reg)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"Iteration {iteration}: Cost = {cost:.4f}\")\n",
    "    \n",
    "    return theta, cost_history\n",
    "\n",
    "# Compare regularized vs non-regularized models\n",
    "lambda_values = [0, 0.01, 0.1, 1.0]\n",
    "models = []\n",
    "\n",
    "for lambda_reg in lambda_values:\n",
    "    theta_reg = np.zeros((X.shape[1], 1))\n",
    "    if lambda_reg == 0:\n",
    "        theta_reg_opt, cost_reg_history = gradient_descent(X, y, theta_reg, alpha, num_iterations)\n",
    "    else:\n",
    "        theta_reg_opt, cost_reg_history = gradient_descent_regularized(X, y, theta_reg, alpha, num_iterations, lambda_reg)\n",
    "    \n",
    "    models.append({\n",
    "        'lambda': lambda_reg,\n",
    "        'theta': theta_reg_opt,\n",
    "        'cost_history': cost_reg_history,\n",
    "        'final_cost': cost_reg_history[-1]\n",
    "    })\n",
    "    \n",
    "    print(f\"Lambda = {lambda_reg}: Final theta = {theta_reg_opt.flatten()}, Final cost = {cost_reg_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Regularization Comparison\n",
    "\n",
    "Let's compare the performance and decision boundaries of regularized vs non-regularized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cost histories for different regularization strengths\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for model in models:\n",
    "    plt.plot(model['cost_history'], label=f'λ = {model[\"lambda\"]}')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost vs Iterations (Different Regularization)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot decision boundaries for different regularization strengths\n",
    "plt.subplot(1, 2, 2)\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "for i, model in enumerate(models):\n",
    "    if i == 0:\n",
    "        plot_decision_boundary(X, y, model['theta'])\n",
    "        plt.title('Decision Boundaries (Different Regularization)')\n",
    "        break  # Only plot the first one to avoid overcrowding\n",
    "\n",
    "# Actually, let's plot them separately for clarity\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot separate decision boundaries\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    plt.sca(axes[i])\n",
    "    plot_decision_boundary(X, y, model['theta'])\n",
    "    plt.title(f'Decision Boundary (λ = {model[\"lambda\"]})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Experimentation and Questions\n",
    "\n",
    "Now let's explore some important concepts:\n",
    "\n",
    "1. **Regularization Effects**: How does increasing λ affect the decision boundary and model parameters?\n",
    "\n",
    "2. **Overfitting Prevention**: Create a more complex dataset (with polynomial features) and see how regularization helps prevent overfitting.\n",
    "\n",
    "3. **Threshold Tuning**: Experiment with different classification thresholds (not just 0.5) and see how it affects precision and recall.\n",
    "\n",
    "4. **Feature Scaling**: How does feature scaling affect logistic regression training?\n",
    "\n",
    "5. **Multi-class Classification**: How would you extend logistic regression for multi-class problems?\n",
    "\n",
    "**Challenge**: Implement polynomial feature expansion and compare regularized vs non-regularized performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Different classification thresholds\n",
    "thresholds = [0.3, 0.5, 0.7]\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_thresh = predict(X, theta_optimized, threshold)\n",
    "    \n",
    "    acc = accuracy_score(y, y_pred_thresh)\n",
    "    prec = precision_score(y, y_pred_thresh)\n",
    "    rec = recall_score(y, y_pred_thresh)\n",
    "    \n",
    "    accuracies.append(acc)\n",
    "    precisions.append(prec)\n",
    "    recalls.append(rec)\n",
    "    \n",
    "    print(f\"Threshold {threshold}: Accuracy={acc:.3f}, Precision={prec:.3f}, Recall={rec:.3f}\")\n",
    "\n",
    "# Plot threshold effects\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, accuracies, 'b-o', label='Accuracy')\n",
    "plt.plot(thresholds, precisions, 'r-o', label='Precision')\n",
    "plt.plot(thresholds, recalls, 'g-o', label='Recall')\n",
    "plt.xlabel('Classification Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision-Recall Tradeoff')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "1. **Logistic Regression** is used for binary classification and outputs probabilities between 0 and 1.\n",
    "\n",
    "2. **Sigmoid Function** maps linear combinations to probabilities, enabling probabilistic predictions.\n",
    "\n",
    "3. **Logistic Cost Function** (cross-entropy) is convex and works well for classification.\n",
    "\n",
    "4. **Regularization** prevents overfitting by penalizing large parameter values.\n",
    "\n",
    "5. **Decision Boundaries** are linear in logistic regression (can be made non-linear with feature engineering).\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the Advanced Learning Algorithms section, we'll learn about neural networks, which can learn complex non-linear decision boundaries automatically without manual feature engineering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
