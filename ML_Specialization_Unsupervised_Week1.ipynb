{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Specialization - Unsupervised Learning\n",
    "## Week 1: Clustering and Anomaly Detection\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand unsupervised learning and its applications\n",
    "- Implement K-means clustering algorithm from scratch\n",
    "- Apply clustering to real datasets and evaluate results\n",
    "- Learn anomaly detection using statistical methods\n",
    "- Implement Gaussian anomaly detection algorithm\n",
    "\n",
    "### Key Concepts:\n",
    "- **Unsupervised Learning**: Learning from unlabeled data\n",
    "- **Clustering**: Grouping similar data points together\n",
    "- **K-means**: Popular clustering algorithm using centroids\n",
    "- **Anomaly Detection**: Finding unusual or outlier data points\n",
    "- **Gaussian Distribution**: Statistical model for anomaly detection\n",
    "\n",
    "Unlike supervised learning, unsupervised methods work without labeled training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Required Libraries\n",
    "\n",
    "Let's import the necessary libraries for our clustering and anomaly detection exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs, make_classification\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from scipy.stats import multivariate_normal\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate Clustering Dataset\n",
    "\n",
    "We'll create synthetic datasets to demonstrate clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic clustering data\n",
    "X_clusters, y_true_clusters = make_blobs(\n",
    "    n_samples=300, \n",
    "    centers=4, \n",
    "    cluster_std=1.5, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Generate data with different cluster shapes\n",
    "X_moons, y_moons = make_classification(\n",
    "    n_samples=300, \n",
    "    n_features=2, \n",
    "    n_informative=2, \n",
    "    n_redundant=0, \n",
    "    n_clusters_per_class=1, \n",
    "    class_sep=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Clusters dataset: X = {X_clusters.shape}\")\n",
    "print(f\"Moons dataset: X = {X_moons.shape}\")\n",
    "print(f\"True cluster labels (clusters): {np.unique(y_true_clusters)}\")\n",
    "\n",
    "# Visualize datasets\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_clusters[:, 0], X_clusters[:, 1], c=y_true_clusters, cmap='viridis', alpha=0.7)\n",
    "plt.title('Well-Separated Clusters')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='True Cluster')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap='viridis', alpha=0.7)\n",
    "plt.title('Non-Spherical Clusters')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='Class')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. K-means Clustering Algorithm\n",
    "\n",
    "K-means is an iterative algorithm that partitions data into K clusters:\n",
    "\n",
    "1. **Initialize**: Randomly choose K cluster centroids\n",
    "2. **Assign**: Assign each point to the nearest centroid\n",
    "3. **Update**: Move centroids to the mean of their assigned points\n",
    "4. **Repeat**: Until convergence or max iterations\n",
    "\n",
    "The objective is to minimize the within-cluster sum of squares (WCSS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_from_scratch(X, K, max_iters=100, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Implement K-means clustering from scratch.\n",
    "    \n",
    "    Args:\n",
    "        X: Data points (n_samples x n_features)\n",
    "        K: Number of clusters\n",
    "        max_iters: Maximum iterations\n",
    "        tol: Tolerance for convergence\n",
    "    \n",
    "    Returns:\n",
    "        centroids: Final cluster centroids\n",
    "        labels: Cluster assignments for each point\n",
    "        wcss_history: Within-cluster sum of squares over iterations\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize centroids randomly\n",
    "    centroids = X[np.random.choice(n_samples, K, replace=False)]\n",
    "    \n",
    "    wcss_history = []\n",
    "    \n",
    "    for iteration in range(max_iters):\n",
    "        # Assign each point to nearest centroid\n",
    "        distances = np.zeros((n_samples, K))\n",
    "        for k in range(K):\n",
    "            distances[:, k] = np.sum((X - centroids[k]) ** 2, axis=1)\n",
    "        \n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        \n",
    "        # Calculate WCSS\n",
    "        wcss = 0\n",
    "        for k in range(K):\n",
    "            cluster_points = X[labels == k]\n",
    "            if len(cluster_points) > 0:\n",
    "                wcss += np.sum((cluster_points - centroids[k]) ** 2)\n",
    "        wcss_history.append(wcss)\n",
    "        \n",
    "        # Update centroids\n",
    "        new_centroids = np.zeros_like(centroids)\n",
    "        for k in range(K):\n",
    "            cluster_points = X[labels == k]\n",
    "            if len(cluster_points) > 0:\n",
    "                new_centroids[k] = np.mean(cluster_points, axis=0)\n",
    "        \n",
    "        # Check convergence\n",
    "        if np.allclose(centroids, new_centroids, atol=tol):\n",
    "            break\n",
    "        \n",
    "        centroids = new_centroids\n",
    "        \n",
    "        if iteration % 10 == 0:\n",
    "            print(f\"Iteration {iteration}: WCSS = {wcss:.4f}\")\n",
    "    \n",
    "    return centroids, labels, wcss_history\n",
    "\n",
    "# Test K-means on our data\n",
    "K = 4\n",
    "centroids, labels, wcss_history = kmeans_from_scratch(X_clusters, K)\n",
    "\n",
    "print(f\"\\nFinal WCSS: {wcss_history[-1]:.4f}\")\n",
    "print(f\"Centroids:\\n{centroids}\")\n",
    "\n",
    "# Visualize K-means results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_clusters[:, 0], X_clusters[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=200, linewidth=3, label='Centroids')\n",
    "plt.title('K-means Clustering Results')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(wcss_history, 'b-o')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Within-Cluster Sum of Squares')\n",
    "plt.title('K-means Convergence')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Choosing K: The Elbow Method\n",
    "\n",
    "How do we choose the optimal number of clusters K?\n",
    "\n",
    "- **Elbow Method**: Plot WCSS vs K, look for the \"elbow\" point\n",
    "- **Silhouette Score**: Measures how similar points are to their own cluster vs other clusters\n",
    "- **Gap Statistic**: Compares within-cluster dispersion to reference distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_k_choice(X, max_K=10):\n",
    "    \"\"\"Evaluate different values of K using multiple metrics\"\"\"\n",
    "    \n",
    "    wcss_values = []\n",
    "    silhouette_scores = []\n",
    "    calinski_scores = []\n",
    "    \n",
    "    K_range = range(2, max_K + 1)\n",
    "    \n",
    "    for K in K_range:\n",
    "        # Run K-means\n",
    "        kmeans = KMeans(n_clusters=K, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        wcss = kmeans.inertia_\n",
    "        silhouette = silhouette_score(X, labels)\n",
    "        calinski = calinski_harabasz_score(X, labels)\n",
    "        \n",
    "        wcss_values.append(wcss)\n",
    "        silhouette_scores.append(silhouette)\n",
    "        calinski_scores.append(calinski)\n",
    "        \n",
    "        print(f\"K = {K}: WCSS = {wcss:.2f}, Silhouette = {silhouette:.3f}, Calinski = {calinski:.2f}\")\n",
    "    \n",
    "    return K_range, wcss_values, silhouette_scores, calinski_scores\n",
    "\n",
    "# Evaluate K for our dataset\n",
    "K_range, wcss_values, silhouette_scores, calinski_scores = evaluate_k_choice(X_clusters)\n",
    "\n",
    "# Plot evaluation metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Elbow method\n",
    "axes[0].plot(K_range, wcss_values, 'bo-')\n",
    "axes[0].set_xlabel('Number of Clusters (K)')\n",
    "axes[0].set_ylabel('Within-Cluster Sum of Squares')\n",
    "axes[0].set_title('Elbow Method')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axvline(x=4, color='red', linestyle='--', alpha=0.7, label='True K=4')\n",
    "axes[0].legend()\n",
    "\n",
    "# Silhouette score\n",
    "axes[1].plot(K_range, silhouette_scores, 'go-')\n",
    "axes[1].set_xlabel('Number of Clusters (K)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Analysis')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axvline(x=4, color='red', linestyle='--', alpha=0.7, label='True K=4')\n",
    "axes[1].legend()\n",
    "\n",
    "# Calinski-Harabasz score\n",
    "axes[2].plot(K_range, calinski_scores, 'ro-')\n",
    "axes[2].set_xlabel('Number of Clusters (K)')\n",
    "axes[2].set_ylabel('Calinski-Harabasz Score')\n",
    "axes[2].set_title('Calinski-Harabasz Index')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].axvline(x=4, color='red', linestyle='--', alpha=0.7, label='True K=4')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal K\n",
    "optimal_k_elbow = 4  # Visually from elbow plot\n",
    "optimal_k_silhouette = K_range[np.argmax(silhouette_scores)]\n",
    "optimal_k_calinski = K_range[np.argmax(calinski_scores)]\n",
    "\n",
    "print(f\"\\nOptimal K suggestions:\")\n",
    "print(f\"Elbow method: K = {optimal_k_elbow}\")\n",
    "print(f\"Silhouette score: K = {optimal_k_silhouette}\")\n",
    "print(f\"Calinski-Harabasz: K = {optimal_k_calinski}\")\n",
    "print(f\"True K: 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Anomaly Detection Fundamentals\n",
    "\n",
    "Anomaly detection identifies data points that deviate significantly from the norm:\n",
    "\n",
    "- **Statistical Methods**: Model data as Gaussian distribution\n",
    "- **Density-based**: Points in low-density regions are anomalies\n",
    "- **Distance-based**: Points far from their neighbors are anomalies\n",
    "- **Model-based**: Train models to distinguish normal from anomalous data\n",
    "\n",
    "We'll focus on the statistical approach using Gaussian distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Gaussian Anomaly Detection\n",
    "\n",
    "The Gaussian approach models each feature as a Gaussian distribution:\n",
    "\n",
    "$$p(x) = \\prod_{j=1}^n p(x_j; \\mu_j, \\sigma_j^2) = \\prod_{j=1}^n \\frac{1}{\\sqrt{2\\pi} \\sigma_j} \\exp\\left(-\\frac{(x_j - \\mu_j)^2}{2\\sigma_j^2}\\right)$$\n",
    "\n",
    "Points with low probability (below threshold ε) are flagged as anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_anomaly_detection(X_train, X_val, epsilon=None):\n",
    "    \"\"\"\n",
    "    Implement Gaussian anomaly detection.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training data (normal examples)\n",
    "        X_val: Validation data for threshold selection\n",
    "        epsilon: Anomaly threshold (if None, will be selected)\n",
    "    \n",
    "    Returns:\n",
    "        mu: Feature means\n",
    "        sigma2: Feature variances\n",
    "        epsilon: Selected threshold\n",
    "    \"\"\"\n",
    "    # Estimate parameters from training data\n",
    "    mu = np.mean(X_train, axis=0)\n",
    "    sigma2 = np.var(X_train, axis=0, ddof=1)  # Use ddof=1 for unbiased estimate\n",
    "    \n",
    "    def multivariate_gaussian(X):\n",
    "        \"\"\"Compute multivariate Gaussian probability density\"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        # Compute probability for each feature independently (diagonal covariance)\n",
    "        probs = np.ones(X.shape[0])\n",
    "        for j in range(n_features):\n",
    "            probs *= (1 / np.sqrt(2 * np.pi * sigma2[j])) * \\\n",
    "                    np.exp(-((X[:, j] - mu[j]) ** 2) / (2 * sigma2[j]))\n",
    "        \n",
    "        return probs\n",
    "    \n",
    "    if epsilon is None:\n",
    "        # Select threshold using validation set\n",
    "        # Assume validation set has some anomalies (we'll simulate this)\n",
    "        y_val = np.zeros(X_val.shape[0])  # Assume all normal for this demo\n",
    "        y_val[:int(0.1 * len(y_val))] = 1  # Mark 10% as anomalies for demo\n",
    "        \n",
    "        # Compute probabilities on validation set\n",
    "        p_val = multivariate_gaussian(X_val)\n",
    "        \n",
    "        # Try different thresholds\n",
    "        best_epsilon = None\n",
    "        best_f1 = 0\n",
    "        \n",
    "        for epsilon_candidate in np.logspace(-20, 0, 100):\n",
    "            predictions = (p_val < epsilon_candidate).astype(int)\n",
    "            \n",
    "            # Calculate F1 score\n",
    "            tp = np.sum((predictions == 1) & (y_val == 1))\n",
    "            fp = np.sum((predictions == 1) & (y_val == 0))\n",
    "            fn = np.sum((predictions == 0) & (y_val == 1))\n",
    "            \n",
    "            if tp + fp > 0 and tp + fn > 0:\n",
    "                precision = tp / (tp + fp)\n",
    "                recall = tp / (tp + fn)\n",
    "                f1 = 2 * precision * recall / (precision + recall)\n",
    "                \n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_epsilon = epsilon_candidate\n",
    "        \n",
    "        epsilon = best_epsilon\n",
    "        print(f\"Selected threshold ε = {epsilon:.2e}, F1 = {best_f1:.4f}\")\n",
    "    \n",
    "    return mu, sigma2, epsilon\n",
    "\n",
    "# Create anomaly detection dataset\n",
    "# Normal data: multivariate Gaussian\n",
    "np.random.seed(42)\n",
    "n_normal = 1000\n",
    "mu_normal = np.array([0, 0])\n",
    "cov_normal = np.array([[1, 0.5], [0.5, 1]])\n",
    "X_normal = np.random.multivariate_normal(mu_normal, cov_normal, n_normal)\n",
    "\n",
    "# Anomalies: points far from the distribution\n",
    "n_anomalies = 50\n",
    "X_anomalies = np.random.uniform(-6, 6, (n_anomalies, 2))\n",
    "# Keep only points that are actually anomalous\n",
    "distances = np.sum((X_anomalies - mu_normal) ** 2, axis=1)\n",
    "anomaly_mask = distances > 9  # Points more than 3 std away\n",
    "X_anomalies = X_anomalies[anomaly_mask[:n_anomalies]]\n",
    "\n",
    "# Combine datasets\n",
    "X_anomaly_data = np.vstack([X_normal, X_anomalies])\n",
    "y_anomaly_true = np.hstack([np.zeros(n_normal), np.ones(len(X_anomalies))])\n",
    "\n",
    "print(f\"Anomaly detection dataset: {X_anomaly_data.shape[0]} samples\")\n",
    "print(f\"Normal samples: {np.sum(y_anomaly_true == 0)}\")\n",
    "print(f\"Anomalies: {np.sum(y_anomaly_true == 1)}\")\n",
    "\n",
    "# Split for training and testing\n",
    "X_train_anomaly = X_normal[:800]  # Only normal data for training\n",
    "X_test_anomaly = X_anomaly_data[800:]\n",
    "y_test_anomaly = y_anomaly_true[800:]\n",
    "\n",
    "# Train anomaly detection model\n",
    "mu, sigma2, epsilon = gaussian_anomaly_detection(X_train_anomaly, X_test_anomaly)\n",
    "\n",
    "# Make predictions\n",
    "def predict_anomalies(X, mu, sigma2, epsilon):\n",
    "    \"\"\"Predict anomalies using trained Gaussian model\"\"\"\n",
    "    n_features = X.shape[1]\n",
    "    probs = np.ones(X.shape[0])\n",
    "    \n",
    "    for j in range(n_features):\n",
    "        probs *= (1 / np.sqrt(2 * np.pi * sigma2[j])) * \\\n",
    "                np.exp(-((X[:, j] - mu[j]) ** 2) / (2 * sigma2[j]))\n",
    "    \n",
    "    return (probs < epsilon).astype(int)\n",
    "\n",
    "y_pred_anomaly = predict_anomalies(X_test_anomaly, mu, sigma2, epsilon)\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"\\nAnomaly Detection Results:\")\n",
    "print(classification_report(y_test_anomaly, y_pred_anomaly, target_names=['Normal', 'Anomaly']))\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_test_anomaly[y_test_anomaly == 0, 0], X_test_anomaly[y_test_anomaly == 0, 1], \n",
    "           c='blue', alpha=0.6, label='True Normal')\n",
    "plt.scatter(X_test_anomaly[y_test_anomaly == 1, 0], X_test_anomaly[y_test_anomaly == 1, 1], \n",
    "           c='red', alpha=0.6, label='True Anomaly')\n",
    "plt.title('True Labels')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_test_anomaly[y_pred_anomaly == 0, 0], X_test_anomaly[y_pred_anomaly == 0, 1], \n",
    "           c='blue', alpha=0.6, label='Predicted Normal')\n",
    "plt.scatter(X_test_anomaly[y_pred_anomaly == 1, 0], X_test_anomaly[y_pred_anomaly == 1, 1], \n",
    "           c='red', alpha=0.6, label='Predicted Anomaly')\n",
    "plt.title('Predictions')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show learned Gaussian parameters\n",
    "print(f\"\\nLearned Parameters:\")\n",
    "print(f\"μ (means): {mu}\")\n",
    "print(f\"σ² (variances): {sigma2}\")\n",
    "print(f\"ε (threshold): {epsilon:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Multivariate Gaussian Anomaly Detection\n",
    "\n",
    "For better performance, we can use the full multivariate Gaussian instead of assuming independent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_gaussian_anomaly_detection(X_train, X_val):\n",
    "    \"\"\"Multivariate Gaussian anomaly detection\"\"\"\n",
    "    \n",
    "    # Estimate parameters\n",
    "    mu = np.mean(X_train, axis=0)\n",
    "    cov = np.cov(X_train.T)\n",
    "    \n",
    "    # Create multivariate Gaussian\n",
    "    rv = multivariate_normal(mu, cov)\n",
    "    \n",
    "    # Select threshold on validation set\n",
    "    p_val = rv.pdf(X_val)\n",
    "    y_val = np.zeros(X_val.shape[0])  # Assume mostly normal\n",
    "    y_val[:int(0.1 * len(y_val))] = 1  # 10% anomalies for demo\n",
    "    \n",
    "    # Find best threshold\n",
    "    best_epsilon = None\n",
    "    best_f1 = 0\n",
    "    \n",
    "    for epsilon in np.logspace(-20, 0, 100):\n",
    "        predictions = (p_val < epsilon).astype(int)\n",
    "        \n",
    "        tp = np.sum((predictions == 1) & (y_val == 1))\n",
    "        fp = np.sum((predictions == 1) & (y_val == 0))\n",
    "        fn = np.sum((predictions == 0) & (y_val == 1))\n",
    "        \n",
    "        if tp + fp > 0 and tp + fn > 0:\n",
    "            precision = tp / (tp + fp)\n",
    "            recall = tp / (tp + fn)\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "            \n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_epsilon = epsilon\n",
    "    \n",
    "    print(f\"Multivariate Gaussian: Selected ε = {best_epsilon:.2e}, F1 = {best_f1:.4f}\")\n",
    "    \n",
    "    return mu, cov, best_epsilon, rv\n",
    "\n",
    "# Train multivariate model\n",
    "mu_mv, cov_mv, epsilon_mv, rv_mv = multivariate_gaussian_anomaly_detection(X_train_anomaly, X_test_anomaly)\n",
    "\n",
    "# Make predictions\n",
    "p_test_mv = rv_mv.pdf(X_test_anomaly)\n",
    "y_pred_mv = (p_test_mv < epsilon_mv).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nMultivariate Gaussian Results:\")\n",
    "print(classification_report(y_test_anomaly, y_pred_mv, target_names=['Normal', 'Anomaly']))\n",
    "\n",
    "# Visualize the Gaussian contours\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Create grid for contour plot\n",
    "x_min, x_max = X_test_anomaly[:, 0].min() - 1, X_test_anomaly[:, 0].max() + 1\n",
    "y_min, y_max = X_test_anomaly[:, 1].min() - 1, X_test_anomaly[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                    np.linspace(y_min, y_max, 100))\n",
    "grid_points = np.column_stack([xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Compute probabilities on grid\n",
    "Z = rv_mv.pdf(grid_points).reshape(xx.shape)\n",
    "\n",
    "# Plot contours\n",
    "plt.contourf(xx, yy, Z, levels=20, cmap='Blues', alpha=0.6)\n",
    "plt.colorbar(label='Probability Density')\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.contour(xx, yy, Z, levels=[epsilon_mv], colors='red', linewidths=2, linestyles='--')\n",
    "\n",
    "# Plot data points\n",
    "plt.scatter(X_test_anomaly[y_test_anomaly == 0, 0], X_test_anomaly[y_test_anomaly == 0, 1], \n",
    "           c='blue', alpha=0.7, label='Normal')\n",
    "plt.scatter(X_test_anomaly[y_test_anomaly == 1, 0], X_test_anomaly[y_test_anomaly == 1, 1], \n",
    "           c='red', alpha=0.7, label='Anomaly')\n",
    "\n",
    "# Mark misclassifications\n",
    "misclassified = y_pred_mv != y_test_anomaly\n",
    "plt.scatter(X_test_anomaly[misclassified, 0], X_test_anomaly[misclassified, 1], \n",
    "           c='yellow', edgecolors='black', s=100, label='Misclassified')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Multivariate Gaussian Anomaly Detection')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Compare covariance matrices\n",
    "print(f\"\\nCovariance Matrix:\\n{cov_mv}\")\n",
    "print(f\"\\nCorrelation: {cov_mv[0,1] / np.sqrt(cov_mv[0,0] * cov_mv[1,1]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Practical Applications and Considerations\n",
    "\n",
    "Let's explore real-world applications and important considerations for unsupervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer segmentation using K-means\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Simulate customer data\n",
    "np.random.seed(42)\n",
    "n_customers = 500\n",
    "\n",
    "# Features: age, income, spending_score\n",
    "customer_data = np.random.multivariate_normal(\n",
    "    [35, 50000, 50], \n",
    "    [[100, 5000, 25], [5000, 1000000, 1000], [25, 1000, 100]], \n",
    "    n_customers\n",
    ")\n",
    "\n",
    "# Add some distinct customer segments\n",
    "young_high_spenders = np.random.multivariate_normal(\n",
    "    [25, 80000, 80], \n",
    "    [[25, 2000, 5], [2000, 400000, 500], [5, 500, 25]], \n",
    "    100\n",
    ")\n",
    "\n",
    "older_low_spenders = np.random.multivariate_normal(\n",
    "    [60, 30000, 20], \n",
    "    [[100, 1000, 2], [1000, 100000, 100], [2, 100, 9]], \n",
    "    100\n",
    ")\n",
    "\n",
    "# Combine all customer data\n",
    "all_customers = np.vstack([customer_data, young_high_spenders, older_low_spenders])\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "customers_scaled = scaler.fit_transform(all_customers)\n",
    "\n",
    "# Apply K-means clustering\n",
    "kmeans_customers = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "customer_clusters = kmeans_customers.fit_predict(customers_scaled)\n",
    "\n",
    "# Visualize customer segments (using first 3 features)\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "scatter = ax.scatter(customers_scaled[:, 0], customers_scaled[:, 1], customers_scaled[:, 2], \n",
    "                    c=customer_clusters, cmap='viridis', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Age (scaled)')\n",
    "ax.set_ylabel('Income (scaled)')\n",
    "ax.set_zlabel('Spending Score (scaled)')\n",
    "ax.set_title('Customer Segmentation using K-means')\n",
    "\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# Analyze cluster characteristics\n",
    "print(\"\\nCustomer Segment Analysis:\")\n",
    "for cluster_id in range(4):\n",
    "    cluster_data = all_customers[customer_clusters == cluster_id]\n",
    "    print(f\"\\nCluster {cluster_id}: {len(cluster_data)} customers\")\n",
    "    print(f\"  Average Age: {cluster_data[:, 0].mean():.1f}\")\n",
    "    print(f\"  Average Income: ${cluster_data[:, 1].mean():,.0f}\")\n",
    "    print(f\"  Average Spending Score: {cluster_data[:, 2].mean():.1f}\")\n",
    "\n",
    "# Evaluate clustering quality\n",
    "silhouette_avg = silhouette_score(customers_scaled, customer_clusters)\n",
    "print(f\"\\nClustering Quality:\")\n",
    "print(f\"Silhouette Score: {silhouette_avg:.3f}\")\n",
    "print(f\"Calinski-Harabasz Score: {calinski_harabasz_score(customers_scaled, customer_clusters):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Experimentation and Questions\n",
    "\n",
    "Now let's explore some advanced clustering and anomaly detection concepts:\n",
    "\n",
    "1. **K-means Initialization**: Compare different initialization methods (random, k-means++, etc.)\n",
    "\n",
    "2. **Distance Metrics**: Experiment with different distance measures for K-means\n",
    "\n",
    "3. **Anomaly Detection Evaluation**: How do you evaluate anomaly detection when you don't have labeled anomalies?\n",
    "\n",
    "4. **Scalability**: How do these algorithms scale with large datasets?\n",
    "\n",
    "5. **Robustness**: How sensitive are these methods to outliers and noise?\n",
    "\n",
    "**Challenge**: Implement a hierarchical clustering algorithm and compare it with K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: K-means sensitivity to initialization\n",
    "def compare_kmeans_initializations(X, K=4, n_runs=10):\n",
    "    \"\"\"Compare different K-means initialization methods\"\"\"\n",
    "    \n",
    "    methods = ['random', 'k-means++']\n",
    "    results = {method: [] for method in methods}\n",
    "    \n",
    "    for method in methods:\n",
    "        print(f\"Testing {method} initialization...\")\n",
    "        for run in range(n_runs):\n",
    "            kmeans = KMeans(n_clusters=K, init=method, n_init=1, random_state=run)\n",
    "            kmeans.fit(X)\n",
    "            results[method].append(kmeans.inertia_)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run initialization comparison\n",
    "init_results = compare_kmeans_initializations(X_clusters)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "methods = list(init_results.keys())\n",
    "data = [init_results[method] for method in methods]\n",
    "\n",
    "plt.boxplot(data, labels=methods)\n",
    "plt.ylabel('Final WCSS')\n",
    "plt.title('K-means Initialization Methods Comparison')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    plt.text(i+1, np.mean(init_results[method]) + 5, f'Mean: {np.mean(init_results[method]):.1f}',\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInitialization Statistics:\")\n",
    "for method in methods:\n",
    "    wcss_values = init_results[method]\n",
    "    print(f\"{method}:\")\n",
    "    print(f\"  Mean WCSS: {np.mean(wcss_values):.2f}\")\n",
    "    print(f\"  Std WCSS: {np.std(wcss_values):.2f}\")\n",
    "    print(f\"  Best WCSS: {np.min(wcss_values):.2f}\")\n",
    "    print(f\"  Worst WCSS: {np.max(wcss_values):.2f}\")\n",
    "\n",
    "# Demonstrate the importance of good initialization\n",
    "print(\"\\nNote: K-means++ typically gives more consistent and better results than random initialization.\")\n",
    "print(\"This is especially important when clusters have similar sizes or are close together.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "1. **K-means Clustering** partitions data into K clusters by minimizing within-cluster distances.\n",
    "\n",
    "2. **Choosing K** is crucial and can be done using elbow method, silhouette analysis, or domain knowledge.\n",
    "\n",
    "3. **Anomaly Detection** identifies unusual data points using statistical modeling (Gaussian approach).\n",
    "\n",
    "4. **Multivariate Gaussian** accounts for feature correlations and often performs better than univariate approach.\n",
    "\n",
    "5. **Initialization Matters** in K-means - K-means++ initialization is generally superior to random initialization.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll explore recommender systems and collaborative filtering, learning how to build personalized recommendation engines that work with user-item interaction data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
