{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Specialization - Advanced Learning Algorithms\n",
    "## Week 3: Model Evaluation and Improvement\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand bias-variance tradeoff in model evaluation\n",
    "- Implement proper data splitting strategies (train/validation/test)\n",
    "- Apply regularization techniques to prevent overfitting\n",
    "- Use cross-validation for robust model evaluation\n",
    "- Analyze learning curves for model diagnosis\n",
    "\n",
    "### Key Concepts:\n",
    "- **Bias-Variance Tradeoff**: Finding the sweet spot between underfitting and overfitting\n",
    "- **Data Splitting**: Train/validation/test sets for proper model evaluation\n",
    "- **Cross-Validation**: Robust evaluation using multiple data splits\n",
    "- **Regularization**: Techniques to prevent overfitting (L1, L2, Dropout)\n",
    "- **Learning Curves**: Diagnostic plots to understand model behavior\n",
    "\n",
    "Building good models requires not just training them, but also properly evaluating and improving their generalization ability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Required Libraries\n",
    "\n",
    "Let's import the necessary libraries for model evaluation exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate Datasets for Evaluation\n",
    "\n",
    "We'll create both regression and classification datasets to demonstrate evaluation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate regression dataset\n",
    "X_reg, y_reg = make_regression(n_samples=1000, n_features=10, noise=20, random_state=42)\n",
    "\n",
    "# Generate classification dataset\n",
    "X_clf, y_clf = make_classification(n_samples=1000, n_features=10, n_informative=8, \n",
    "                                   n_redundant=2, random_state=42)\n",
    "\n",
    "print(f\"Regression dataset: X = {X_reg.shape}, y = {y_reg.shape}\")\n",
    "print(f\"Classification dataset: X = {X_clf.shape}, y = {y_clf.shape}\")\n",
    "print(f\"Classification classes: {np.unique(y_clf)}\")\n",
    "\n",
    "# Split both datasets\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "X_reg_train, X_reg_val, y_reg_train, y_reg_val = train_test_split(\n",
    "    X_reg_train, y_reg_train, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
    "\n",
    "X_clf_train, X_clf_test, y_clf_train, y_clf_test = train_test_split(\n",
    "    X_clf, y_clf, test_size=0.2, random_state=42)\n",
    "X_clf_train, X_clf_val, y_clf_train, y_clf_val = train_test_split(\n",
    "    X_clf_train, y_clf_train, test_size=0.25, random_state=42)\n",
    "\n",
    "print(f\"\\nRegression splits:\")\n",
    "print(f\"  Train: {X_reg_train.shape[0]} samples\")\n",
    "print(f\"  Validation: {X_reg_val.shape[0]} samples\")\n",
    "print(f\"  Test: {X_reg_test.shape[0]} samples\")\n",
    "\n",
    "print(f\"\\nClassification splits:\")\n",
    "print(f\"  Train: {X_clf_train.shape[0]} samples\")\n",
    "print(f\"  Validation: {X_clf_val.shape[0]} samples\")\n",
    "print(f\"  Test: {X_clf_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Understanding Bias-Variance Tradeoff\n",
    "\n",
    "The bias-variance tradeoff is fundamental to machine learning:\n",
    "\n",
    "- **Bias**: Error due to overly simplistic assumptions (underfitting)\n",
    "- **Variance**: Error due to sensitivity to training data variations (overfitting)\n",
    "- **Total Error**: Bias² + Variance + Irreducible Error\n",
    "\n",
    "We want to find the sweet spot that minimizes total error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polynomial_features(X, degree):\n",
    "    \"\"\"Create polynomial features up to specified degree\"\"\"\n",
    "    X_poly = X.copy()\n",
    "    for d in range(2, degree + 1):\n",
    "        for i in range(X.shape[1]):\n",
    "            X_poly = np.column_stack([X_poly, X[:, i]**d])\n",
    "    return X_poly\n",
    "\n",
    "# Demonstrate bias-variance tradeoff with polynomial regression\n",
    "degrees = [1, 2, 3, 4, 8, 12]\n",
    "train_errors = []\n",
    "val_errors = []\n",
    "test_errors = []\n",
    "\n",
    "# Use only first feature for simplicity\n",
    "X_simple = X_reg_train[:, :1]\n",
    "X_val_simple = X_reg_val[:, :1]\n",
    "X_test_simple = X_reg_test[:, :1]\n",
    "\n",
    "for degree in degrees:\n",
    "    # Create polynomial features\n",
    "    X_train_poly = create_polynomial_features(X_simple, degree)\n",
    "    X_val_poly = create_polynomial_features(X_val_simple, degree)\n",
    "    X_test_poly = create_polynomial_features(X_test_simple, degree)\n",
    "    \n",
    "    # Train linear regression\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_reg_train)\n",
    "    \n",
    "    # Calculate errors\n",
    "    train_pred = model.predict(X_train_poly)\n",
    "    val_pred = model.predict(X_val_poly)\n",
    "    test_pred = model.predict(X_test_poly)\n",
    "    \n",
    "    train_error = mean_squared_error(y_reg_train, train_pred)\n",
    "    val_error = mean_squared_error(y_reg_val, val_pred)\n",
    "    test_error = mean_squared_error(y_reg_test, test_pred)\n",
    "    \n",
    "    train_errors.append(train_error)\n",
    "    val_errors.append(val_error)\n",
    "    test_errors.append(test_error)\n",
    "    \n",
    "    print(f\"Degree {degree}: Train MSE = {train_error:.2f}, Val MSE = {val_error:.2f}, Test MSE = {test_error:.2f}\")\n",
    "\n",
    "# Plot bias-variance tradeoff\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(degrees, train_errors, 'b-o', label='Training Error')\n",
    "plt.plot(degrees, val_errors, 'r-o', label='Validation Error')\n",
    "plt.plot(degrees, test_errors, 'g-o', label='Test Error')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Bias-Variance Tradeoff')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Visualize overfitting with degree 12\n",
    "plt.subplot(1, 2, 2)\n",
    "degree_overfit = 12\n",
    "X_plot = np.linspace(X_simple.min(), X_simple.max(), 100).reshape(-1, 1)\n",
    "X_plot_poly = create_polynomial_features(X_plot, degree_overfit)\n",
    "\n",
    "model_overfit = LinearRegression()\n",
    "model_overfit.fit(create_polynomial_features(X_simple, degree_overfit), y_reg_train)\n",
    "y_plot = model_overfit.predict(X_plot_poly)\n",
    "\n",
    "plt.scatter(X_simple, y_reg_train, alpha=0.6, label='Training data')\n",
    "plt.plot(X_plot, y_plot, 'r-', linewidth=2, label='Overfitted model')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Overfitting Example (Degree 12)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Learning Curves\n",
    "\n",
    "Learning curves show how model performance changes with training set size:\n",
    "- **High bias**: Both training and validation error are high and close together\n",
    "- **High variance**: Large gap between training and validation error\n",
    "- **Good fit**: Both errors are low and close together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(X_train, y_train, X_val, y_val, model_func, **model_kwargs):\n",
    "    \"\"\"\n",
    "    Plot learning curves for a given model.\n",
    "    \n",
    "    Args:\n",
    "        X_train, y_train: Training data\n",
    "        X_val, y_val: Validation data\n",
    "        model_func: Function to create model\n",
    "        model_kwargs: Arguments for model creation\n",
    "    \"\"\"\n",
    "    train_sizes = np.arange(0.1, 1.1, 0.1)  # 10%, 20%, ..., 100%\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "    \n",
    "    for train_size in train_sizes:\n",
    "        # Use subset of training data\n",
    "        n_samples = int(train_size * len(X_train))\n",
    "        X_subset = X_train[:n_samples]\n",
    "        y_subset = y_train[:n_samples]\n",
    "        \n",
    "        # Train model\n",
    "        model = model_func(**model_kwargs)\n",
    "        model.fit(X_subset, y_subset)\n",
    "        \n",
    "        # Calculate errors\n",
    "        train_pred = model.predict(X_subset)\n",
    "        val_pred = model.predict(X_val)\n",
    "        \n",
    "        if len(y_train.shape) > 1 and y_train.shape[1] > 1:  # Classification\n",
    "            train_error = 1 - accuracy_score(y_subset, np.argmax(train_pred, axis=1))\n",
    "            val_error = 1 - accuracy_score(y_val, np.argmax(val_pred, axis=1))\n",
    "        else:  # Regression\n",
    "            train_error = mean_squared_error(y_subset, train_pred)\n",
    "            val_error = mean_squared_error(y_val, val_pred)\n",
    "        \n",
    "        train_errors.append(train_error)\n",
    "        val_errors.append(val_error)\n",
    "    \n",
    "    # Plot\n",
    "    plt.plot(train_sizes * 100, train_errors, 'b-o', label='Training Error')\n",
    "    plt.plot(train_sizes * 100, val_errors, 'r-o', label='Validation Error')\n",
    "    plt.xlabel('Training Set Size (%)')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title('Learning Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot learning curves for regression\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_learning_curves(X_reg_train, y_reg_train, X_reg_val, y_reg_val, LinearRegression)\n",
    "plt.title('Learning Curves - Linear Regression (Good Fit)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# High variance example: Neural network on small dataset\n",
    "def create_complex_model():\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_reg_train.shape[1],)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "plot_learning_curves(X_reg_train[:200], y_reg_train[:200], X_reg_val, y_reg_val, create_complex_model)\n",
    "plt.title('Learning Curves - Complex Model (High Variance)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Regularization Techniques\n",
    "\n",
    "Regularization adds penalty terms to prevent overfitting:\n",
    "\n",
    "- **L2 Regularization (Ridge)**: Penalizes large weights, shrinks them towards zero\n",
    "- **L1 Regularization (Lasso)**: Can force some weights to exactly zero (feature selection)\n",
    "- **Dropout**: Randomly deactivates neurons during training\n",
    "- **Early Stopping**: Stop training when validation error stops improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare regularization techniques on regression\n",
    "alpha_values = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "# Ridge (L2) regularization\n",
    "ridge_train_errors = []\n",
    "ridge_val_errors = []\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_reg_train, y_reg_train)\n",
    "    \n",
    "    train_pred = ridge.predict(X_reg_train)\n",
    "    val_pred = ridge.predict(X_reg_val)\n",
    "    \n",
    "    train_error = mean_squared_error(y_reg_train, train_pred)\n",
    "    val_error = mean_squared_error(y_reg_val, val_pred)\n",
    "    \n",
    "    ridge_train_errors.append(train_error)\n",
    "    ridge_val_errors.append(val_error)\n",
    "\n",
    "# Lasso (L1) regularization\n",
    "lasso_train_errors = []\n",
    "lasso_val_errors = []\n",
    "lasso_sparsity = []\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    lasso.fit(X_reg_train, y_reg_train)\n",
    "    \n",
    "    train_pred = lasso.predict(X_reg_train)\n",
    "    val_pred = lasso.predict(X_reg_val)\n",
    "    \n",
    "    train_error = mean_squared_error(y_reg_train, train_pred)\n",
    "    val_error = mean_squared_error(y_reg_val, val_pred)\n",
    "    sparsity = np.sum(np.abs(lasso.coef_) < 1e-4) / len(lasso.coef_)\n",
    "    \n",
    "    lasso_train_errors.append(train_error)\n",
    "    lasso_val_errors.append(val_error)\n",
    "    lasso_sparsity.append(sparsity)\n",
    "\n",
    "# Plot regularization comparison\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(alpha_values, ridge_train_errors, 'b-o', label='Ridge Training')\n",
    "plt.plot(alpha_values, ridge_val_errors, 'b--o', label='Ridge Validation')\n",
    "plt.plot(alpha_values, lasso_train_errors, 'r-o', label='Lasso Training')\n",
    "plt.plot(alpha_values, lasso_val_errors, 'r--o', label='Lasso Validation')\n",
    "plt.xlabel('Regularization Strength (α)')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Regularization Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(alpha_values, lasso_sparsity, 'g-o')\n",
    "plt.xlabel('Lasso Regularization Strength (α)')\n",
    "plt.ylabel('Fraction of Zero Coefficients')\n",
    "plt.title('Lasso Feature Selection')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xscale('log')\n",
    "\n",
    "# Neural network with dropout\n",
    "plt.subplot(1, 3, 3)\n",
    "dropout_rates = [0.0, 0.1, 0.2, 0.3, 0.5]\n",
    "dropout_accuracies = []\n",
    "\n",
    "for dropout_rate in dropout_rates:\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_clf_train.shape[1],)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Use early stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    model.fit(X_clf_train, y_clf_train, epochs=100, batch_size=32, \n",
    "              validation_data=(X_clf_val, y_clf_val), callbacks=[early_stopping], verbose=0)\n",
    "    \n",
    "    _, accuracy = model.evaluate(X_clf_test, y_clf_test, verbose=0)\n",
    "    dropout_accuracies.append(accuracy)\n",
    "\n",
    "plt.plot(dropout_rates, dropout_accuracies, 'm-o')\n",
    "plt.xlabel('Dropout Rate')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Dropout Regularization')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDropout results:\")\n",
    "for rate, acc in zip(dropout_rates, dropout_accuracies):\n",
    "    print(f\"Dropout {rate}: Test Accuracy = {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Cross-Validation\n",
    "\n",
    "Cross-validation provides more robust model evaluation by using multiple train/validation splits:\n",
    "\n",
    "- **K-Fold CV**: Divide data into K folds, train on K-1, validate on 1, repeat K times\n",
    "- **Stratified K-Fold**: Maintains class distribution in each fold\n",
    "- **Leave-One-Out**: Extreme case where K = N (expensive for large datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement K-fold cross-validation manually\n",
    "def cross_validate_model(X, y, model_func, k=5, **model_kwargs):\n",
    "    \"\"\"\n",
    "    Perform K-fold cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        X, y: Full dataset\n",
    "        model_func: Function to create model\n",
    "        k: Number of folds\n",
    "        model_kwargs: Model parameters\n",
    "    \n",
    "    Returns:\n",
    "        scores: List of validation scores for each fold\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    fold = 1\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Train model\n",
    "        model = model_func(**model_kwargs)\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Evaluate\n",
    "        val_pred = model.predict(X_val_fold)\n",
    "        if hasattr(model, 'predict_proba'):  # Classification\n",
    "            score = accuracy_score(y_val_fold, np.argmax(val_pred, axis=1) if val_pred.ndim > 1 else val_pred)\n",
    "        else:  # Regression\n",
    "            score = -mean_squared_error(y_val_fold, val_pred)  # Negative MSE for sklearn compatibility\n",
    "        \n",
    "        scores.append(score)\n",
    "        print(f\"Fold {fold}: Score = {score:.4f}\")\n",
    "        fold += 1\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Test cross-validation on regression\n",
    "print(\"Regression Cross-Validation:\")\n",
    "reg_scores = cross_validate_model(X_reg_train, y_reg_train, LinearRegression, k=5)\n",
    "print(f\"Mean CV Score: {np.mean(reg_scores):.4f} (+/- {np.std(reg_scores) * 2:.4f})\")\n",
    "\n",
    "# Neural network cross-validation (simplified)\n",
    "def create_nn_model():\n",
    "    model = Sequential([\n",
    "        Dense(32, activation='relu', input_shape=(X_clf_train.shape[1],)),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print(\"\\nClassification Cross-Validation:\")\n",
    "# For neural networks, we'll use a simpler approach due to training time\n",
    "clf_scores = cross_val_score(create_nn_model(), X_clf_train, y_clf_train, cv=3, scoring='accuracy')\n",
    "print(f\"CV Scores: {clf_scores}\")\n",
    "print(f\"Mean CV Accuracy: {np.mean(clf_scores):.4f} (+/- {np.std(clf_scores) * 2:.4f})\")\n",
    "\n",
    "# Compare models using cross-validation\n",
    "models_to_compare = {\n",
    "    'Linear Regression': LinearRegression,\n",
    "    'Ridge (α=0.1)': lambda: Ridge(alpha=0.1),\n",
    "    'Lasso (α=0.1)': lambda: Lasso(alpha=0.1)\n",
    "}\n",
    "\n",
    "cv_results = {}\n",
    "for name, model_func in models_to_compare.items():\n",
    "    scores = cross_val_score(model_func(), X_reg_train, y_reg_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_results[name] = -scores  # Convert back to positive MSE\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  CV MSE: {np.mean(-scores):.4f} (+/- {np.std(-scores) * 2:.4f})\")\n",
    "\n",
    "# Plot CV results\n",
    "plt.figure(figsize=(10, 6))\n",
    "model_names = list(cv_results.keys())\n",
    "means = [np.mean(scores) for scores in cv_results.values()]\n",
    "stds = [np.std(scores) for scores in cv_results.values()]\n",
    "\n",
    "plt.bar(model_names, means, yerr=stds, capsize=5, alpha=0.7)\n",
    "plt.ylabel('Cross-Validation MSE')\n",
    "plt.title('Model Comparison with Cross-Validation')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Data Augmentation\n",
    "\n",
    "Data augmentation artificially increases dataset size by creating modified versions of existing data:\n",
    "\n",
    "- **Noise Injection**: Add small random noise to features\n",
    "- **Feature Mixing**: Combine features from different samples\n",
    "- **Synthetic Samples**: Generate new samples based on existing data distribution\n",
    "\n",
    "This is especially useful when training data is limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_regression_data(X, y, noise_factor=0.1, mix_factor=0.1, n_augmented=None):\n",
    "    \"\"\"\n",
    "    Augment regression data with noise injection and feature mixing.\n",
    "    \n",
    "    Args:\n",
    "        X, y: Original data\n",
    "        noise_factor: Amount of noise to add\n",
    "        mix_factor: Amount of feature mixing\n",
    "        n_augmented: Number of augmented samples to generate\n",
    "    \n",
    "    Returns:\n",
    "        X_aug, y_aug: Augmented data\n",
    "    \"\"\"\n",
    "    if n_augmented is None:\n",
    "        n_augmented = len(X)\n",
    "    \n",
    "    X_aug = []\n",
    "    y_aug = []\n",
    "    \n",
    "    for _ in range(n_augmented):\n",
    "        # Randomly select two samples to mix\n",
    "        idx1, idx2 = np.random.choice(len(X), 2, replace=False)\n",
    "        \n",
    "        # Mix features\n",
    "        mix_ratio = np.random.uniform(0.3, 0.7)\n",
    "        X_mixed = mix_ratio * X[idx1] + (1 - mix_ratio) * X[idx2]\n",
    "        y_mixed = mix_ratio * y[idx1] + (1 - mix_ratio) * y[idx2]\n",
    "        \n",
    "        # Add noise\n",
    "        X_noise = X_mixed + np.random.normal(0, noise_factor * np.std(X, axis=0), X.shape[1])\n",
    "        y_noise = y_mixed + np.random.normal(0, noise_factor * np.std(y))\n",
    "        \n",
    "        X_aug.append(X_noise)\n",
    "        y_aug.append(y_noise)\n",
    "    \n",
    "    return np.array(X_aug), np.array(y_aug)\n",
    "\n",
    "# Demonstrate data augmentation impact\n",
    "original_sizes = [50, 100, 200, 500]\n",
    "augmentation_factors = [0, 1, 2]  # 0x, 1x, 2x augmentation\n",
    "\n",
    "results_augmentation = {}\n",
    "\n",
    "for orig_size in original_sizes:\n",
    "    X_subset = X_reg_train[:orig_size]\n",
    "    y_subset = y_reg_train[:orig_size]\n",
    "    \n",
    "    for factor in augmentation_factors:\n",
    "        # Create augmented dataset\n",
    "        if factor > 0:\n",
    "            X_aug, y_aug = augment_regression_data(X_subset, y_subset, \n",
    "                                                   n_augmented=factor * len(X_subset))\n",
    "            X_combined = np.vstack([X_subset, X_aug])\n",
    "            y_combined = np.concatenate([y_subset, y_aug])\n",
    "        else:\n",
    "            X_combined = X_subset\n",
    "            y_combined = y_subset\n",
    "        \n",
    "        # Train and evaluate\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_combined, y_combined)\n",
    "        \n",
    "        val_pred = model.predict(X_reg_val)\n",
    "        val_error = mean_squared_error(y_reg_val, val_pred)\n",
    "        \n",
    "        key = f\"Size {orig_size}, Aug {factor}x\"\n",
    "        results_augmentation[key] = {\n",
    "            'size': len(X_combined),\n",
    "            'val_error': val_error\n",
    "        }\n",
    "        \n",
    "        print(f\"{key} (total {len(X_combined)} samples): Val MSE = {val_error:.4f}\")\n",
    "\n",
    "# Plot augmentation results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, orig_size in enumerate(original_sizes):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    \n",
    "    factors = []\n",
    "    errors = []\n",
    "    sizes = []\n",
    "    \n",
    "    for factor in augmentation_factors:\n",
    "        key = f\"Size {orig_size}, Aug {factor}x\"\n",
    "        result = results_augmentation[key]\n",
    "        factors.append(factor)\n",
    "        errors.append(result['val_error'])\n",
    "        sizes.append(result['size'])\n",
    "    \n",
    "    plt.plot(factors, errors, 'b-o')\n",
    "    plt.xlabel('Augmentation Factor')\n",
    "    plt.ylabel('Validation MSE')\n",
    "    plt.title(f'Data Augmentation (Original Size: {orig_size})')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Model Selection and Final Evaluation\n",
    "\n",
    "Let's put it all together: proper model selection and final evaluation on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection based on validation performance\n",
    "def select_best_model(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Select the best model based on validation performance\"\"\"\n",
    "    \n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge (α=0.01)': Ridge(alpha=0.01),\n",
    "        'Ridge (α=0.1)': Ridge(alpha=0.1),\n",
    "        'Lasso (α=0.01)': Lasso(alpha=0.01),\n",
    "        'Lasso (α=0.1)': Lasso(alpha=0.1)\n",
    "    }\n",
    "    \n",
    "    best_model = None\n",
    "    best_score = float('inf')\n",
    "    best_name = None\n",
    "    \n",
    "    print(\"Model Selection Results:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        val_pred = model.predict(X_val)\n",
    "        val_score = mean_squared_error(y_val, val_pred)\n",
    "        \n",
    "        print(f\"{name}: Validation MSE = {val_score:.4f}\")\n",
    "        \n",
    "        if val_score < best_score:\n",
    "            best_score = val_score\n",
    "            best_model = model\n",
    "            best_name = name\n",
    "    \n",
    "    print(f\"\\nBest model: {best_name} (MSE = {best_score:.4f})\")\n",
    "    return best_model, best_name\n",
    "\n",
    "# Select best regression model\n",
    "best_reg_model, best_reg_name = select_best_model(X_reg_train, y_reg_train, X_reg_val, y_reg_val)\n",
    "\n",
    "# Final evaluation on test set\n",
    "test_pred_reg = best_reg_model.predict(X_reg_test)\n",
    "test_mse_reg = mean_squared_error(y_reg_test, test_pred_reg)\n",
    "\n",
    "print(f\"\\nFinal Test Evaluation for {best_reg_name}:\")\n",
    "print(f\"Test MSE: {test_mse_reg:.4f}\")\n",
    "print(f\"Test RMSE: {np.sqrt(test_mse_reg):.4f}\")\n",
    "\n",
    "# Neural network selection for classification\n",
    "def select_best_nn(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Select best neural network architecture\"\"\"\n",
    "    \n",
    "    architectures = [\n",
    "        {'name': 'Small', 'layers': [16, 8]},\n",
    "        {'name': 'Medium', 'layers': [32, 16]},\n",
    "        {'name': 'Large', 'layers': [64, 32, 16]},\n",
    "        {'name': 'Regularized', 'layers': [32, 16], 'dropout': 0.2}\n",
    "    ]\n",
    "    \n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "    best_name = None\n",
    "    \n",
    "    print(\"\\nNeural Network Selection Results:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for arch in architectures:\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Build layers\n",
    "        for i, units in enumerate(arch['layers']):\n",
    "            if i == 0:\n",
    "                model.add(Dense(units, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "            else:\n",
    "                model.add(Dense(units, activation='relu'))\n",
    "            \n",
    "            # Add dropout if specified\n",
    "            if 'dropout' in arch and i < len(arch['layers']) - 1:\n",
    "                model.add(Dropout(arch['dropout']))\n",
    "        \n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Train\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, epochs=100, batch_size=32, \n",
    "                 validation_data=(X_val, y_val), callbacks=[early_stopping], verbose=0)\n",
    "        \n",
    "        # Evaluate\n",
    "        _, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"{arch['name']}: Validation Accuracy = {val_accuracy:.4f}\")\n",
    "        \n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
            best_model = model\n",
    "            best_name = arch['name']\n",
    "    \n",
    "    print(f\"\\nBest neural network: {best_name} (Accuracy = {best_accuracy:.4f})\")\n",
    "    return best_model, best_name\n",
    "\n",
    "# Select best neural network\n",
    "best_nn_model, best_nn_name = select_best_nn(X_clf_train, y_clf_train, X_clf_val, y_clf_val)\n",
    "\n",
    "# Final evaluation\n",
    "_, test_accuracy_nn = best_nn_model.evaluate(X_clf_test, y_clf_test, verbose=0)\n",
    "y_pred_nn = (best_nn_model.predict(X_clf_test) > 0.5).astype(int).flatten()\n",
    "\n",
    "print(f\"\\nFinal Test Evaluation for {best_nn_name} Neural Network:\")\n",
    "print(f\"Test Accuracy: {test_accuracy_nn:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_clf_test, y_pred_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Experimentation and Questions\n",
    "\n",
    "Now let's explore some advanced evaluation concepts:\n",
    "\n",
    "1. **Error Analysis**: Examine specific types of errors your model makes. Are there patterns?\n",
    "\n",
    "2. **Confidence Intervals**: Calculate confidence intervals for your model performance estimates.\n",
    "\n",
    "3. **Model Calibration**: For classification, ensure predicted probabilities are well-calibrated.\n",
    "\n",
    "4. **Robustness Testing**: Test your model on slightly corrupted or noisy data.\n",
    "\n",
    "5. **Computational Cost**: Compare training time and inference time for different models.\n",
    "\n",
    "**Challenge**: Implement a complete machine learning pipeline with proper validation, hyperparameter tuning, and model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Model calibration\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_proba = best_nn_model.predict(X_clf_test).flatten()\n",
    "\n",
    "# Calculate calibration curve\n",
    "prob_true, prob_pred = calibration_curve(y_clf_test, y_proba, n_bins=10)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(prob_pred, prob_true, 's-', label='Neural Network')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('True Probability')\n",
    "plt.title('Calibration Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot probability distributions\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(y_proba[y_clf_test == 0], alpha=0.7, label='Class 0', bins=20)\n",
    "plt.hist(y_proba[y_clf_test == 1], alpha=0.7, label='Class 1', bins=20)\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Probability Distributions by Class')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Error analysis\n",
    "errors = y_clf_test != y_pred_nn\n",
    "error_indices = np.where(errors)[0]\n",
    "\n",
    "print(f\"Number of errors: {len(error_indices)} out of {len(y_clf_test)} ({len(error_indices)/len(y_clf_test)*100:.2f}%)\")\n",
    "\n",
    "# Analyze error patterns\n",
    "if len(error_indices) > 0:\n",
    "    print(\"\\nError Analysis:\")\n",
    "    print(f\"Average confidence in wrong predictions: {np.mean(y_proba[error_indices]):.3f}\")\n",
    "    print(f\"False positives: {np.sum((y_pred_nn == 1) & (y_clf_test == 0))}\")\n",
    "    print(f\"False negatives: {np.sum((y_pred_nn == 0) & (y_clf_test == 1))}\")\n",
    "\n",
    "# Feature importance for errors\n",
    "if len(error_indices) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.boxplot([X_clf_test[y_clf_test == 0, :].mean(axis=0),\n",
    "                 X_clf_test[y_clf_test == 1, :].mean(axis=0),\n",
    "                 X_clf_test[error_indices, :].mean(axis=0)], \n",
    "                labels=['Correct Class 0', 'Correct Class 1', 'Errors'])\n",
    "    plt.ylabel('Feature Values')\n",
    "    plt.title('Feature Distributions: Correct vs Incorrect Predictions')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "1. **Bias-Variance Tradeoff** is fundamental: balance underfitting (high bias) and overfitting (high variance).\n",
    "\n",
    "2. **Proper Data Splitting** (train/validation/test) ensures honest model evaluation.\n",
    "\n",
    "3. **Learning Curves** diagnose whether a model has high bias or high variance.\n",
    "\n",
    "4. **Regularization** (L1, L2, Dropout) prevents overfitting by penalizing complexity.\n",
    "\n",
    "5. **Cross-Validation** provides robust performance estimates with limited data.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll explore decision trees and ensemble methods, which offer different approaches to machine learning that are often more interpretable and can handle various types of data effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
