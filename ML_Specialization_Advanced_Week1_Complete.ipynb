{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Specialization - Advanced Learning Algorithms\n",
    "## Week 1: Neural Networks Basics - Complete Solutions with Explanations\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand the basics of neural networks\n",
    "- Build a simple neural network for binary classification\n",
    "- Learn about activation functions and forward propagation\n",
    "- Implement neural networks using TensorFlow\n",
    "\n",
    "### Key Concepts:\n",
    "- **Neural Networks**: Composed of layers of interconnected nodes (neurons)\n",
    "- **Forward Propagation**: How information flows through the network\n",
    "- **Activation Functions**: Non-linear functions that enable learning complex patterns\n",
    "- **Hidden Layers**: Intermediate layers between input and output\n",
    "- **TensorFlow**: A popular deep learning framework\n",
    "\n",
    "Neural networks can automatically learn complex patterns and decision boundaries, making them more powerful than logistic regression for many tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Required Libraries\n",
    "\n",
    "Let's import the necessary libraries for our neural network exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, make_circles\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate Non-Linear Classification Dataset\n",
    "\n",
    "We'll create a dataset that requires non-linear decision boundaries, which logistic regression cannot handle well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate non-linear binary classification data (circular pattern)\n",
    "X, y = make_circles(n_samples=500, noise=0.1, factor=0.5, random_state=42)\n",
    "\n",
    "print(f\"Dataset shape: X = {X.shape}, y = {y.shape}\")\n",
    "print(f\"Classes: {np.unique(y)}\")\n",
    "print(f\"Class distribution: Class 0 = {np.sum(y == 0)}, Class 1 = {np.sum(y == 1)}\")\n",
    "print(f\"First 5 samples:\")\n",
    "for i in range(5):\n",
    "    print(f\"Features: [{X[i, 0]:.2f}, {X[i, 1]:.2f}], Label: {y[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualize the Non-Linear Data\n",
    "\n",
    "Let's plot our data to see why linear models won't work well here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], color='blue', alpha=0.7, label='Class 0')\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], color='red', alpha=0.7, label='Class 1')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Non-Linear Classification Dataset')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='bwr', alpha=0.7)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='bwr', alpha=0.7, edgecolors='black', linewidths=2)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Train (filled) vs Test (outlined)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Neural Network Architecture\n",
    "\n",
    "A neural network consists of:\n",
    "- **Input Layer**: Receives the features\n",
    "- **Hidden Layers**: Learn intermediate representations\n",
    "- **Output Layer**: Produces the final prediction\n",
    "\n",
    "For our binary classification task, we'll use:\n",
    "- Input layer: 2 neurons (for 2 features)\n",
    "- Hidden layer: 4 neurons with ReLU activation\n",
    "- Output layer: 1 neuron with sigmoid activation\n",
    "\n",
    "The network can be represented as:\n",
    "\n",
    "**Forward Propagation:**\n",
    "$$a^{[1]} = \\sigma(W^{[1]} x + b^{[1]})$$\n",
    "$$\\hat{y} = \\sigma(W^{[2]} a^{[1]} + b^{[2]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Build Neural Network with TensorFlow\n",
    "\n",
    "**Exercise 5.1: Build Your First Neural Network**\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Learn how to use TensorFlow's Sequential API\n",
    "- Understand the structure of neural networks (input → hidden → output layers)\n",
    "- Implement appropriate activation functions for each layer\n",
    "- Configure the model for binary classification\n",
    "\n",
    "**Task:**\n",
    "Complete the neural network model below by replacing the `None` values with appropriate Dense layers. Use:\n",
    "- 4 neurons in the hidden layer with ReLU activation\n",
    "- 1 neuron in the output layer with sigmoid activation\n",
    "- Proper input_shape parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network model\n",
    "model = Sequential([\n",
    "    # YOUR CODE HERE - Add layers to the neural network\n",
    "    # Hint: Use Dense layers with appropriate activation functions\n",
    "    # Input layer -> Hidden layer -> Output layer\n",
    "    \n",
    "    # Input layer (implicitly created by first Dense layer)\n",
    "    None,  # Replace with Dense layer for hidden layer\n",
    "    \n",
    "    # Output layer\n",
    "    None   # Replace with Dense layer for output\n",
    "], name='binary_classifier')\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',  # Adaptive learning rate optimization\n",
    "    loss='binary_crossentropy',  # Appropriate for binary classification\n",
    "    metrics=['accuracy']  # Track accuracy during training\n",
    ")\n",
    "\n",
    "print(\"Model compiled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**: Here's the complete neural network implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network model\n",
    "model = Sequential([\n",
    "    # Input layer (implicitly created by first Dense layer)\n",
    "    Dense(4, activation='relu', input_shape=(2,), name='hidden_layer'),  # Hidden layer with 4 neurons\n",
    "    \n",
    "    # Output layer with 1 neuron and sigmoid activation for binary classification\n",
    "    Dense(1, activation='sigmoid', name='output_layer')\n",
    "], name='binary_classifier')\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',  # Adaptive learning rate optimization\n",
    "    loss='binary_crossentropy',  # Appropriate for binary classification\n",
    "    metrics=['accuracy']  # Track accuracy during training\n",
    ")\n",
    "\n",
    "print(\"Model compiled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution Explanation:**\n",
    "- We created a neural network with one hidden layer containing 4 neurons using ReLU activation\n",
    "- The output layer has 1 neuron with sigmoid activation for binary classification\n",
    "- We used the Sequential API which stacks layers linearly\n",
    "- The `input_shape=(2,)` parameter tells Keras the input has 2 features\n",
    "- Adam optimizer adapts the learning rate during training\n",
    "- Binary cross-entropy is the standard loss for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Train the Neural Network\n",
    "\n",
    "Now let's train our neural network on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,  # Number of training iterations\n",
    "    batch_size=32,  # Number of samples per gradient update\n",
    "    validation_split=0.2,  # Use 20% of training data for validation\n",
    "    verbose=1  # Show training progress\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Evaluate Model Performance\n",
    "\n",
    "Let's evaluate our trained neural network on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Visualize Training History\n",
    "\n",
    "Let's plot the training and validation metrics over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Visualize Decision Boundary\n",
    "\n",
    "Let's visualize the complex decision boundary learned by our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary_nn(X, y, model):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary for a neural network.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Target values\n",
    "        model: Trained neural network model\n",
    "    \"\"\"\n",
    "    # Create a mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                        np.linspace(y_min, y_max, 100))\n",
    "    \n",
    "    # Make predictions on the grid\n",
    "    grid_points = np.column_stack([xx.ravel(), yy.ravel()])\n",
    "    Z = model.predict(grid_points, verbose=0)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot the decision boundary and data\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu_r', levels=np.linspace(0, 1, 11))\n",
    "    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "    \n",
    "    # Plot the data points\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], color='blue', alpha=0.8, label='Class 0', edgecolors='white')\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], color='red', alpha=0.8, label='Class 1', edgecolors='white')\n",
    "    \n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title('Neural Network Decision Boundary')\n",
    "    plt.legend()\n",
    "    plt.colorbar(label='Predicted Probability')\n",
    "    plt.axis('equal')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_decision_boundary_nn(X_test, y_test, model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Compare with Logistic Regression\n",
    "\n",
    "Let's compare our neural network with a simple logistic regression model to see the improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regression model for comparison\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate logistic regression\n",
    "log_reg_accuracy = log_reg.score(X_test, y_test)\n",
    "print(f\"Logistic Regression Test Accuracy: {log_reg_accuracy:.4f}\")\n",
    "print(f\"Neural Network Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Improvement: {((test_accuracy - log_reg_accuracy) / log_reg_accuracy * 100):.2f}%\")\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "# Logistic regression decision boundary\n",
    "x_min, x_max = X_test[:, 0].min() - 0.5, X_test[:, 0].max() + 0.5\n",
    "y_min, y_max = X_test[:, 1].min() - 0.5, X_test[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                    np.linspace(y_min, y_max, 100))\n",
    "grid_points = np.column_stack([xx.ravel(), yy.ravel()])\n",
    "Z_log = log_reg.predict_proba(grid_points)[:, 1].reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z_log, alpha=0.4, cmap='RdYlBu_r')\n",
    "plt.contour(xx, yy, Z_log, levels=[0.5], colors='black', linewidths=2)\n",
    "plt.scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1], color='blue', alpha=0.8, label='Class 0')\n",
    "plt.scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1], color='red', alpha=0.8, label='Class 1')\n",
    "plt.title('Logistic Regression Decision Boundary')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Neural network decision boundary (already plotted above, but let's show it again)\n",
    "plot_decision_boundary_nn(X_test, y_test, model)\n",
    "plt.title('Neural Network Decision Boundary')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Experiment with Network Architecture\n",
    "\n",
    "Let's experiment with different neural network architectures to see how they affect performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different architectures\n",
    "architectures = [\n",
    "    {'name': 'Small Network', 'hidden_units': [2], 'epochs': 100},\n",
    "    {'name': 'Medium Network', 'hidden_units': [4], 'epochs': 100},\n",
    "    {'name': 'Large Network', 'hidden_units': [8, 4], 'epochs': 100},\n",
    "    {'name': 'Deep Network', 'hidden_units': [16, 8, 4], 'epochs': 150}\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for arch in architectures:\n",
    "    print(f\"\\nTraining {arch['name']}...\")\n",
    "    \n",
    "    # Build model\n",
    "    model_exp = Sequential(name=arch['name'])\n",
    "    \n",
    "    # Add hidden layers\n",
    "    for i, units in enumerate(arch['hidden_units']):\n",
    "        if i == 0:\n",
    "            model_exp.add(Dense(units, activation='relu', input_shape=(2,)))\n",
    "        else:\n",
    "            model_exp.add(Dense(units, activation='relu'))\n",
    "    \n",
    "    # Output layer\n",
    "    model_exp.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile\n",
    "    model_exp.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train\n",
    "    history_exp = model_exp.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=arch['epochs'],\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss_exp, test_acc_exp = model_exp.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    results.append({\n",
    "        'name': arch['name'],\n",
    "        'test_accuracy': test_acc_exp,\n",
    "        'final_val_accuracy': history_exp.history['val_accuracy'][-1],\n",
    "        'model': model_exp\n",
    "    })\n",
    "    \n",
    "    print(f\"{arch['name']}: Test Accuracy = {test_acc_exp:.4f}\")\n",
    "\n",
    "# Plot architecture comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "names = [r['name'] for r in results]\n",
    "accuracies = [r['test_accuracy'] for r in results]\n",
    "\n",
    "plt.bar(names, accuracies, color=['lightblue', 'blue', 'darkblue', 'navy'])\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Model Performance vs Architecture Complexity')\n",
    "plt.ylim(0.8, 1.0)\n",
    "for i, acc in enumerate(accuracies):\n",
    "    plt.text(i, acc + 0.005, f'{acc:.3f}', ha='center')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Manual Neural Network Implementation\n",
    "\n",
    "**Exercise 12.1: Implement Neural Network from Scratch**\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the mathematical foundations of neural networks\n",
    "- Implement forward propagation manually\n",
    "- Learn about parameter initialization\n",
    "- Compare manual implementation with high-level frameworks\n",
    "\n",
    "**Background:**\n",
    "Neural networks perform matrix operations under the hood. Understanding these operations helps you:\n",
    "- Debug complex models\n",
    "- Optimize performance\n",
    "- Choose appropriate architectures\n",
    "- Implement custom layers and loss functions\n",
    "\n",
    "**Task:**\n",
    "Complete the manual neural network implementation below. You'll need to:\n",
    "1. Initialize weights and biases with small random values\n",
    "2. Implement forward propagation using matrix multiplication\n",
    "3. Use ReLU activation for hidden layer and sigmoid for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"ReLU activation function\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    \"\"\"\n",
    "    Initialize neural network parameters.\n",
    "    \n",
    "    Args:\n",
    "        input_size: Number of input features\n",
    "        hidden_size: Number of hidden units\n",
    "        output_size: Number of output units\n",
    "    \n",
    "    Returns:\n",
    "        parameters: Dictionary containing W1, b1, W2, b2\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE - Initialize weights and biases\n",
    "    # Hint: Use random initialization with small values\n",
    "    W1 = None  # Replace with weight matrix for first layer\n",
    "    b1 = None  # Replace with bias vector for first layer\n",
    "    W2 = None  # Replace with weight matrix for second layer\n",
    "    b2 = None  # Replace with bias vector for second layer\n",
    "    \n",
    "    parameters = {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2\n",
    "    }\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Perform forward propagation.\n",
    "    \n",
    "    Args:\n",
    "        X: Input features (m x input_size)\n",
    "        parameters: Network parameters\n",
    "    \n",
    "    Returns:\n",
    "        A2: Output predictions\n",
    "        cache: Intermediate values for backpropagation\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE - Implement forward propagation\n",
    "    # Hint: Z1 = X @ W1 + b1, A1 = relu(Z1), Z2 = A1 @ W2 + b2, A2 = sigmoid(Z2)\n",
    "    \n",
    "    W1, b1, W2, b2 = parameters['W1'], parameters['b1'], parameters['W2'], parameters['b2']\n",
    "    \n",
    "    Z1 = None  # Replace with linear combination for first layer\n",
    "    A1 = None  # Replace with activation for first layer\n",
    "    Z2 = None  # Replace with linear combination for second layer\n",
    "    A2 = None  # Replace with activation for second layer\n",
    "    \n",
    "    cache = {\n",
    "        'Z1': Z1,\n",
    "        'A1': A1,\n",
    "        'Z2': Z2,\n",
    "        'A2': A2\n",
    "    }\n",
    "    \n",
    "    return A2, cache\n",
    "\n",
    "# Test the manual implementation\n",
    "parameters = initialize_parameters(input_size=2, hidden_size=4, output_size=1)\n",
    "predictions_manual, _ = forward_propagation(X_test[:5], parameters)\n",
    "print(f\"Manual neural network predictions (first 5 samples): {predictions_manual.flatten()}\")\n",
    "\n",
    "# Compare with TensorFlow model\n",
    "tf_predictions = model.predict(X_test[:5], verbose=0)\n",
    "print(f\"TensorFlow model predictions (first 5 samples): {tf_predictions.flatten()}\")\n",
    "print(\"\\n(Note: Random initialization means predictions will be different, but the structure is the same)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**: Here's the complete manual neural network implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"ReLU activation function\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    \"\"\"\n",
    "    Initialize neural network parameters.\n",
    "    \n",
    "    Args:\n",
    "        input_size: Number of input features\n",
    "        hidden_size: Number of hidden units\n",
    "        output_size: Number of output units\n",
    "    \n",
    "    Returns:\n",
    "        parameters: Dictionary containing W1, b1, W2, b2\n",
    "    \"\"\"\n",
    "    # Initialize weights and biases with small random values\n",
    "    W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "    b1 = np.zeros((1, hidden_size))\n",
    "    W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "    b2 = np.zeros((1, output_size))\n",
    "    \n",
    "    parameters = {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2\n",
    "    }\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Perform forward propagation.\n",
    "    \n",
    "    Args:\n",
    "        X: Input features (m x input_size)\n",
    "        parameters: Network parameters\n",
    "    \n",
    "    Returns:\n",
    "        A2: Output predictions\n",
    "        cache: Intermediate values for backpropagation\n",
    "    \"\"\"\n",
    "    # Retrieve parameters\n",
    "    W1, b1, W2, b2 = parameters['W1'], parameters['b1'], parameters['W2'], parameters['b2']\n",
    "    \n",
    "    # Forward propagation\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    cache = {\n",
    "        'Z1': Z1,\n",
    "        'A1': A1,\n",
    "        'Z2': Z2,\n",
    "        'A2': A2\n",
    "    }\n",
    "    \n",
    "    return A2, cache\n",
    "\n",
    "# Test the manual implementation\n",
    "parameters = initialize_parameters(input_size=2, hidden_size=4, output_size=1)\n",
    "predictions_manual, _ = forward_propagation(X_test[:5], parameters)\n",
    "print(f\"Manual neural network predictions (first 5 samples): {predictions_manual.flatten()}\")\n",
    "\n",
    "# Compare with TensorFlow model\n",
    "tf_predictions = model.predict(X_test[:5], verbose=0)\n",
    "print(f\"TensorFlow model predictions (first 5 samples): {tf_predictions.flatten()}\")\n",
    "print(\"\\n(Note: Random initialization means predictions will be different, but the structure is the same)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution Explanation:**\n",
    "- **Parameter Initialization**: Weights are initialized with small random values (scaled by 0.01) to break symmetry, biases are initialized to zeros\n",
    "- **Forward Propagation**: \n",
    "  - Z1 = X·W1 + b1 (linear combination for first layer)\n",
    "  - A1 = ReLU(Z1) (apply activation function)\n",
    "  - Z2 = A1·W2 + b2 (linear combination for output layer)\n",
    "  - A2 = sigmoid(Z2) (final prediction probabilities)\n",
    "- The manual implementation shows exactly what TensorFlow/Keras does under the hood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Experimentation and Advanced Concepts\n",
    "\n",
    "**Exercise 13.1: Activation Function Comparison**\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Compare different activation functions and their effects on learning\n",
    "- Understand the vanishing gradient problem\n",
    "- Learn about advanced activation functions\n",
    "\n",
    "**Task:**\n",
    "Experiment with different activation functions (ReLU, tanh, sigmoid) and analyze their performance differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution and Analysis:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Different activation functions\n",
    "activation_functions = ['relu', 'tanh', 'sigmoid']\n",
    "activation_results = []\n",
    "\n",
    "for activation in activation_functions:\n",
    "    print(f\"\\nTesting {activation} activation...\")\n",
    "    \n",
    "    # Build model with different activation\n",
    "    model_act = Sequential([\n",
    "        Dense(8, activation=activation, input_shape=(2,)),\n",
    "        Dense(4, activation=activation),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model_act.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train\n",
    "    history_act = model_act.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss_act, test_acc_act = model_act.evaluate(X_test, y_test, verbose=0)\n",
    "    activation_results.append({\n",
    "        'activation': activation,\n",
    "        'test_accuracy': test_acc_act,\n",
    "        'history': history_act\n",
    "    })\n",
    "    \n",
    "    print(f\"{activation}: Test Accuracy = {test_acc_act:.4f}\")\n",
    "\n",
    "# Plot activation function comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "activations = [r['activation'] for r in activation_results]\n",
    "accuracies = [r['test_accuracy'] for r in activation_results]\n",
    "\n",
    "plt.bar(activations, accuracies, color=['lightgreen', 'green', 'darkgreen'])\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Model Performance vs Activation Function')\n",
    "plt.ylim(0.85, 1.0)\n",
    "for i, acc in enumerate(accuracies):\n",
    "    plt.text(i, acc + 0.002, f'{acc:.3f}', ha='center')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activation Function Analysis:**\n",
    "- **ReLU**: Usually performs best, addresses vanishing gradient problem, computationally efficient\n",
    "- **Tanh**: Better than sigmoid (zero-centered), but still suffers from vanishing gradients\n",
    "- **Sigmoid**: Can cause vanishing gradients in deep networks, less commonly used in hidden layers\n",
    "\n",
    "**Key Insights:**\n",
    "- ReLU helps with the vanishing gradient problem\n",
    "- Sigmoid/tanh compress outputs to specific ranges\n",
    "- Choice of activation function affects convergence speed and final performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "1. **Neural Networks** can learn complex non-linear decision boundaries automatically.\n",
    "\n",
    "2. **Forward Propagation** passes information through layers using matrix multiplications and activation functions.\n",
    "\n",
    "3. **Activation Functions** like ReLU enable learning of non-linear patterns.\n",
    "\n",
    "4. **Hidden Layers** allow the network to learn hierarchical features.\n",
    "\n",
    "5. **TensorFlow/Keras** provides high-level APIs that make building neural networks easy.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll explore multi-class classification problems and learn about the softmax function, cross-entropy loss, and more advanced activation functions.\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) by Michael Nielsen\n",
    "- [Deep Learning Book](https://www.deeplearningbook.org/) by Ian Goodfellow et al.\n",
    "- [TensorFlow Documentation](https://www.tensorflow.org/guide/keras)\n",
    "\n",
    "### Practice Exercises\n",
    "\n",
    "1. Try implementing a neural network for the XOR problem (a classic non-linear problem).\n",
    "2. Experiment with different optimizers (SGD, RMSprop, Adam) and compare their performance.\n",
    "3. Add regularization techniques (dropout, L2 regularization) to prevent overfitting.\n",
    "4. Implement early stopping based on validation performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
