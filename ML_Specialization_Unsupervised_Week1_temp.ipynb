{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Specialization - Unsupervised Learning\n",
    "## Week 1: Clustering and Anomaly Detection\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand unsupervised learning and its applications\n",
    "- Implement K-means clustering algorithm from scratch\n",
    "- Apply clustering to real datasets and evaluate results\n",
    "- Learn anomaly detection using statistical methods\n",
    "- Implement Gaussian anomaly detection algorithm\n",
    "\n",
    "### Key Concepts:\n",
    "- **Unsupervised Learning**: Learning from unlabeled data\n",
    "- **Clustering**: Grouping similar data points together\n",
    "- **K-means**: Popular clustering algorithm using centroids\n",
    "- **Anomaly Detection**: Finding unusual or outlier data points\n",
    "- **Gaussian Distribution**: Statistical model for anomaly detection\n",
    "\n",
    "Unlike supervised learning, unsupervised methods work without labeled training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Required Libraries\n",
    "\n",
    "Let's import the necessary libraries for our clustering and anomaly detection exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs, make_classification\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from scipy.stats import multivariate_normal\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate Clustering Dataset\n",
    "\n",
    "We'll create synthetic datasets to demonstrate clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE - Generate synthetic clustering data\n",
    "# Hint: Use make_blobs for well-separated clusters and make_classification for non-spherical clusters\n",
    "# Create variables: X_clusters, y_true_clusters, X_moons, y_moons\n",
    "\n",
    "X_clusters = None  # Replace with your code\n",
    "y_true_clusters = None  # Replace with your code\n",
    "X_moons = None  # Replace with your code\n",
    "y_moons = None  # Replace with your code\n",
    "\n",
    "print(f\"Clusters dataset: X = {X_clusters.shape if X_clusters is not None else 'Not created'}\")\n",
    "print(f\"Moons dataset: X = {X_moons.shape if X_moons is not None else 'Not created'}\")\n",
    "\n",
    "# YOUR CODE HERE - Visualize the datasets\n",
    "# Hint: Create a figure with two subplots showing both datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. K-means Clustering Algorithm\n",
    "\n",
    "K-means is an iterative algorithm that partitions data into K clusters:\n",
    "\n",
    "1. **Initialize**: Randomly choose K cluster centroids\n",
    "2. **Assign**: Assign each point to the nearest centroid\n",
    "3. **Update**: Move centroids to the mean of their assigned points\n",
    "4. **Repeat**: Until convergence or max iterations\n",
    "\n",
    "The objective is to minimize the within-cluster sum of squares (WCSS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_from_scratch(X, K, max_iters=100, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Implement K-means clustering from scratch.\n",
    "    \n",
    "    Args:\n",
    "        X: Data points (n_samples x n_features)\n",
    "        K: Number of clusters\n",
    "        max_iters: Maximum iterations\n",
    "        tol: Tolerance for convergence\n",
    "    \n",
    "    Returns:\n",
    "        centroids: Final cluster centroids\n",
    "        labels: Cluster assignments for each point\n",
    "        wcss_history: Within-cluster sum of squares over iterations\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE - Implement K-means from scratch\n",
    "    # Hint: \n",
    "    # 1. Initialize centroids randomly\n",
    "    # 2. For each iteration:\n",
    "    #    a. Assign points to nearest centroids\n",
    "    #    b. Update centroids to cluster means\n",
    "    #    c. Check for convergence\n",
    "    # 3. Track WCSS over iterations\n",
    "    \n",
    "    centroids = None  # Replace with your implementation\n",
    "    labels = None     # Replace with your implementation\n",
    "    wcss_history = [] # Replace with your implementation\n",
    "    \n",
    "    return centroids, labels, wcss_history\n",
    "\n",
    "# Test your K-means implementation\n",
    "if X_clusters is not None:\n",
    "    K = 4\n",
    "    centroids, labels, wcss_history = kmeans_from_scratch(X_clusters, K)\n",
    "    \n",
    "    print(f\"K-means completed. Final WCSS: {wcss_history[-1] if wcss_history else 'Not implemented'}\")\n",
    "    \n",
    "    # YOUR CODE HERE - Visualize K-means results\n",
    "    # Hint: Plot the data points colored by cluster assignments and centroids\n",
    "else:\n",
    "    print(\"Please complete Exercise 2 first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Choosing K: The Elbow Method\n",
    "\n",
    "How do we choose the optimal number of clusters K?\n",
    "\n",
    "- **Elbow Method**: Plot WCSS vs K, look for the \"elbow\" point\n",
    "- **Silhouette Score**: Measures how similar points are to their own cluster vs other clusters\n",
    "- **Gap Statistic**: Compares within-cluster dispersion to reference distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_k_choice(X, max_K=10):\n",
    "    \"\"\"Evaluate different values of K using multiple metrics\"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE - Implement evaluation of different K values\n",
    "    # Hint: For each K, run K-means and calculate WCSS, silhouette score, etc.\n",
    "    \n",
    "    wcss_values = []     # Replace with your implementation\n",
    "    silhouette_scores = []  # Replace with your implementation\n",
    "    calinski_scores = []    # Replace with your implementation\n",
    "    K_range = range(2, max_K + 1)\n",
    "    \n",
    "    return K_range, wcss_values, silhouette_scores, calinski_scores\n",
    "\n",
    "# Evaluate K for our dataset\n",
    "if X_clusters is not None:\n",
    "    K_range, wcss_values, silhouette_scores, calinski_scores = evaluate_k_choice(X_clusters)\n",
    "    \n",
    "    # YOUR CODE HERE - Plot evaluation metrics\n",
    "    # Hint: Create subplots showing elbow method, silhouette scores, etc.\n",
    "else:\n",
    "    print(\"Please complete Exercise 2 first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Anomaly Detection Fundamentals\n",
    "\n",
    "Anomaly detection identifies data points that deviate significantly from the norm:\n",
    "\n",
    "- **Statistical Methods**: Model data as Gaussian distribution\n",
    "- **Density-based**: Points in low-density regions are anomalies\n",
    "- **Distance-based**: Points far from their neighbors are anomalies\n",
    "- **Model-based**: Train models to distinguish normal from anomalous data\n",
    "\n",
    "We'll focus on the statistical approach using Gaussian distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Gaussian Anomaly Detection\n",
    "\n",
    "The Gaussian approach models each feature as a Gaussian distribution:\n",
    "\n",
    "$$p(x) = \\prod_{j=1}^n p(x_j; \\mu_j, \\sigma_j^2) = \\prod_{j=1}^n \\frac{1}{\\sqrt{2\\pi} \\sigma_j} \\exp\\left(-\\frac{(x_j - \\mu_j)^2}{2\\sigma_j^2}\\right)$$\n",
    "\n",
    "Points with low probability (below threshold Îµ) are flagged as anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_anomaly_detection(X_train, X_val, epsilon=None):\n",
    "    \"\"\"\n",
    "    Implement Gaussian anomaly detection.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training data (normal examples)\n",
    "        X_val: Validation data for threshold selection\n",
    "        epsilon: Anomaly threshold (if None, will be selected)\n",
    "    \n",
    "    Returns:\n",
    "        mu: Feature means\n",
    "        sigma2: Feature variances\n",
    "        epsilon: Selected threshold\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE - Implement Gaussian anomaly detection\n",
    "    # Hint:\n",
    "    # 1. Estimate parameters (mu, sigma2) from training data\n",
    "    # 2. Compute probabilities for validation set\n",
    "    # 3. Select threshold epsilon using F1 score optimization\n",
    "    \n",
    "    mu = None      # Replace with your implementation\n",
    "    sigma2 = None  # Replace with your implementation\n",
    "    epsilon = None # Replace with your implementation\n",
    "    \n",
    "    return mu, sigma2, epsilon\n",
    "\n",
    "# YOUR CODE HERE - Create anomaly detection dataset and test your implementation\n",
    "# Hint: Generate normal data and add some anomalies, then train and evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Multivariate Gaussian Anomaly Detection\n",
    "\n",
    "For better performance, we can use the full multivariate Gaussian instead of assuming independent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_gaussian_anomaly_detection(X_train, X_val):\n",
    "    \"\"\"Multivariate Gaussian anomaly detection\"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE - Implement multivariate Gaussian anomaly detection\n",
    "    # Hint:\n",
    "    # 1. Estimate mean vector and covariance matrix\n",
    "    # 2. Use multivariate_normal from scipy.stats\n",
    "    # 3. Select threshold on validation set\n",
    "    \n",
    "    mu = None    # Replace with your implementation\n",
    "    cov = None   # Replace with your implementation\n",
    "    epsilon = None  # Replace with your implementation\n",
    "    rv = None   # Replace with your implementation\n",
    "    \n",
    "    return mu, cov, epsilon, rv\n",
    "\n",
    "# YOUR CODE HERE - Test multivariate Gaussian anomaly detection\n",
    "# Hint: Compare with the univariate approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Practical Applications and Considerations\n",
    "\n",
    "Let's explore real-world applications and important considerations for unsupervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE - Implement a practical application\n",
    "# Hint: Try customer segmentation using K-means or anomaly detection on a real dataset\n",
    "# Example: Customer segmentation using the customer data we created\n",
    "\n",
    "# Generate customer data (you can modify this)\n",
    "np.random.seed(42)\n",
    "n_customers = 500\n",
    "\n",
    "# YOUR CODE HERE - Create customer features and apply clustering\n",
    "# Hint: Use K-means to segment customers based on their features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Experimentation and Questions\n",
    "\n",
    "Now let's explore some advanced clustering and anomaly detection concepts:\n",
    "\n",
    "1. **K-means Initialization**: Compare different initialization methods (random, k-means++, etc.)\n",
    "\n",
    "2. **Distance Metrics**: Experiment with different distance measures for K-means\n",
    "\n",
    "3. **Anomaly Detection Evaluation**: How do you evaluate anomaly detection when you don't have labeled anomalies?\n",
    "\n",
    "4. **Scalability**: How do these algorithms scale with large datasets?\n",
    "\n",
    "5. **Robustness**: How sensitive are these methods to outliers and noise?\n",
    "\n",
    "**Challenge**: Implement a hierarchical clustering algorithm and compare it with K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE - Experiment with K-means initialization methods\n",
    "# Hint: Compare 'random' vs 'k-means++' initialization\n",
    "\n",
    "def compare_kmeans_initializations(X, K=4, n_runs=10):\n",
    "    \"\"\"Compare different K-means initialization methods\"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE - Implement comparison of initialization methods\n",
    "    # Hint: Run K-means multiple times with different initialization methods\n",
    "    \n",
    "    methods = ['random', 'k-means++']\n",
    "    results = {method: [] for method in methods}\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test your implementation\n",
    "if X_clusters is not None:\n",
    "    init_results = compare_kmeans_initializations(X_clusters)\n",
    "    \n",
    "    # YOUR CODE HERE - Visualize and analyze the results\n",
    "    # Hint: Create box plots comparing the different methods\n",
    "else:\n",
    "    print(\"Please complete Exercise 2 first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "1. **K-means Clustering** partitions data into K clusters by minimizing within-cluster distances.\n",
    "\n",
    "2. **Choosing K** is crucial and can be done using elbow method, silhouette analysis, or domain knowledge.\n",
    "\n",
    "3. **Anomaly Detection** identifies unusual data points using statistical modeling (Gaussian approach).\n",
    "\n",
    "4. **Multivariate Gaussian** accounts for feature correlations and often performs better than univariate approach.\n",
    "\n",
    "5. **Initialization Matters** in K-means - K-means++ initialization is generally superior to random initialization.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll explore recommender systems and collaborative filtering, learning how to build personalized recommendation engines that work with user-item interaction data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
