{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Specialization - Supervised Learning: Regression and Classification\n",
    "## Week 1: Linear Regression with One Variable - SOLUTIONS\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand the basics of linear regression\n",
    "- Implement and compute cost functions\n",
    "- Apply gradient descent algorithm\n",
    "- Build intuition through hands-on implementation\n",
    "\n",
    "### Key Concepts:\n",
    "- **Linear Regression**: A method to predict continuous values using a linear relationship\n",
    "- **Cost Function**: Measures how well our model fits the data (Mean Squared Error)\n",
    "- **Gradient Descent**: An optimization algorithm to minimize the cost function\n",
    "\n",
    "In this notebook, you'll see complete working implementations of linear regression from scratch using NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Required Libraries\n",
    "\n",
    "Let's start by importing the necessary libraries for our exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate Synthetic Data\n",
    "\n",
    "We'll create a simple dataset to work with. This represents a real-world scenario where we want to predict house prices based on their size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for house prices vs size\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# Add intercept term (x0 = 1) to X\n",
    "X = np.column_stack([np.ones(X.shape[0]), X])\n",
    "\n",
    "# Reshape y to be a column vector\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "print(f\"Dataset shape: X = {X.shape}, y = {y.shape}\")\n",
    "print(f\"First 5 samples:\")\n",
    "for i in range(5):\n",
    "    print(f\"House size: {X[i, 1]:.2f}, Price: {y[i, 0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualize the Data\n",
    "\n",
    "Let's plot our data to understand the relationship between house size and price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 1], y, alpha=0.7, color='blue')\n",
    "plt.xlabel('House Size (normalized)')\n",
    "plt.ylabel('House Price')\n",
    "plt.title('House Price vs Size')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Linear Regression Model\n",
    "\n",
    "The linear regression model can be expressed as:\n",
    "\n",
    "$$h_\\theta(x) = \\theta_0 + \\theta_1 x$$\n",
    "\n",
    "Or in matrix form:\n",
    "\n",
    "$$h_\\theta(X) = X \\theta$$\n",
    "\n",
    "Where:\n",
    "- $X$ is the feature matrix (with intercept column)\n",
    "- $\\theta$ is the parameter vector\n",
    "- $h_\\theta(X)$ is our prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, theta):\n",
    "    \"\"\"\n",
    "    Make predictions using linear regression model.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix (m x n)\n",
    "        theta: Parameter vector (n x 1)\n",
    "    \n",
    "    Returns:\n",
    "        predictions: Predicted values (m x 1)\n",
    "    \"\"\"\n",
    "    return np.dot(X, theta)\n",
    "\n",
    "# Test the function with random theta\n",
    "theta_test = np.array([[2.0], [1.5]])\n",
    "predictions_test = predict(X[:5], theta_test)\n",
    "print(f\"Test predictions: {predictions_test.flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Cost Function\n",
    "\n",
    "The cost function measures how well our model fits the data. For linear regression, we use the Mean Squared Error (MSE):\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "Where:\n",
    "- $m$ is the number of training examples\n",
    "- $h_\\theta(x^{(i)})$ is the prediction for the i-th example\n",
    "- $y^{(i)}$ is the actual value for the i-th example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the cost function for linear regression.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix (m x n)\n",
    "        y: Target values (m x 1)\n",
    "        theta: Parameter vector (n x 1)\n",
    "    \n",
    "    Returns:\n",
    "        cost: The cost value (scalar)\n",
    "    \"\"\"\n",
    "    m = len(y)  # number of training examples\n",
    "    \n",
    "    predictions = predict(X, theta)\n",
    "    errors = predictions - y\n",
    "    cost = (1 / (2 * m)) * np.sum(errors ** 2)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "# Test the cost function\n",
    "theta_test = np.array([[2.0], [1.5]])\n",
    "cost_test = compute_cost(X, y, theta_test)\n",
    "print(f\"Cost with theta = {theta_test.flatten()}: {cost_test:.4f}\")\n",
    "\n",
    "# Test with different theta values\n",
    "theta_good = np.array([[0.0], [2.0]])\n",
    "cost_good = compute_cost(X, y, theta_good)\n",
    "print(f\"Cost with better theta = {theta_good.flatten()}: {cost_good:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Gradient Descent\n",
    "\n",
    "Gradient descent is used to minimize the cost function. The algorithm updates parameters simultaneously:\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta)$$\n",
    "\n",
    "For linear regression, the partial derivative is:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ is the learning rate\n",
    "- $x_j^{(i)}$ is the j-th feature of the i-th training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, num_iterations):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to learn theta.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix (m x n)\n",
    "        y: Target values (m x 1)\n",
    "        theta: Initial parameter vector (n x 1)\n",
    "        alpha: Learning rate\n",
    "        num_iterations: Number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        theta: Optimized parameter vector\n",
    "        cost_history: List of cost values over iterations\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        predictions = predict(X, theta)\n",
    "        errors = predictions - y\n",
    "        gradient = (1 / m) * np.dot(X.T, errors)\n",
    "        theta = theta - alpha * gradient\n",
    "        \n",
    "        cost = compute_cost(X, y, theta)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"Iteration {iteration}: Cost = {cost:.4f}\")\n",
    "    \n",
    "    return theta, cost_history\n",
    "\n",
    "# Initialize theta with zeros\n",
    "theta_initial = np.zeros((X.shape[1], 1))\n",
    "\n",
    "# Set hyperparameters\n",
    "alpha = 0.01\n",
    "num_iterations = 1000\n",
    "\n",
    "# Run gradient descent\n",
    "theta_optimized, cost_history = gradient_descent(X, y, theta_initial, alpha, num_iterations)\n",
    "\n",
    "print(f\"\\nOptimized theta: {theta_optimized.flatten()}\")\n",
    "print(f\"Final cost: {cost_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Visualize the Results\n",
    "\n",
    "Let's plot the cost function over iterations and the fitted line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cost vs iterations\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost Function vs Iterations')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot fitted line\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X[:, 1], y, alpha=0.7, color='blue', label='Data points')\n",
    "\n",
    "# Generate predictions for the fitted line\n",
    "X_plot = np.linspace(X[:, 1].min(), X[:, 1].max(), 100).reshape(-1, 1)\n",
    "X_plot_with_intercept = np.column_stack([np.ones(X_plot.shape[0]), X_plot])\n",
    "y_plot = predict(X_plot_with_intercept, theta_optimized)\n",
    "\n",
    "plt.plot(X_plot, y_plot, color='red', linewidth=2, label='Fitted line')\n",
    "plt.xlabel('House Size')\n",
    "plt.ylabel('House Price')\n",
    "plt.title('Linear Regression Fit')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Model Evaluation\n",
    "\n",
    "Let's evaluate our model using Mean Squared Error and visualize the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the entire dataset\n",
    "y_pred = predict(X, theta_optimized)\n",
    "\n",
    "# Calculate MSE\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "# Calculate R-squared (coefficient of determination)\n",
    "ss_res = np.sum((y - y_pred) ** 2)\n",
    "ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "r_squared = 1 - (ss_res / ss_tot)\n",
    "print(f\"R-squared: {r_squared:.4f}\")\n",
    "\n",
    "# Plot residuals\n",
    "residuals = y - y_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.7, color='green')\n",
    "plt.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Experimentation and Solutions\n",
    "\n",
    "Now that you've implemented linear regression from scratch, let's explore some important concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 9.1 Solution: Learning Rate Effects\n",
    "print(\"=== Exercise 9.1: Learning Rate Effects ===\")\n",
    "alphas = [0.001, 0.01, 0.1]\n",
    "cost_histories = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    theta_temp = np.zeros((X.shape[1], 1))\n",
    "    _, cost_history = gradient_descent(X, y, theta_temp, alpha, 500)\n",
    "    cost_histories.append(cost_history)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, alpha in enumerate(alphas):\n",
    "    plt.plot(cost_histories[i], label=f'α = {alpha}')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost Function for Different Learning Rates')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Analysis: Learning rate 0.01 converges fastest to the minimum.\")\n",
    "print(\"Learning rate 0.001 is too slow, 0.1 might overshoot and oscillate.\")\n",
    "\n",
    "# Exercise 9.2 Solution: Feature Scaling\n",
    "print(\"\\n=== Exercise 9.2: Feature Scaling ===\")\n",
    "\n",
    "def feature_normalize(X):\n",
    "    \"\"\"Normalize features using z-score normalization\"\"\"\n",
    "    mu = np.mean(X, axis=0)\n",
    "    sigma = np.std(X, axis=0, ddof=1)\n",
    "    X_norm = (X - mu) / sigma\n",
    "    return X_norm, mu, sigma\n",
    "\n",
    "# Create features with different scales\n",
    "X_unscaled = np.random.randn(100, 2)\n",
    "X_unscaled[:, 1] = X_unscaled[:, 1] * 1000  # Very different scale\n",
    "y_test = X_unscaled[:, 0] * 2 + X_unscaled[:, 1] * 0.001 + np.random.randn(100) * 0.1\n",
    "\n",
    "X_unscaled = np.column_stack([np.ones(X_unscaled.shape[0]), X_unscaled])\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "# Test without scaling\n",
    "theta_unscaled = np.zeros((X_unscaled.shape[1], 1))\n",
    "theta_unscaled_opt, _ = gradient_descent(X_unscaled, y_test, theta_unscaled, 0.01, 1000)\n",
    "\n",
    "# Test with scaling\n",
    "X_scaled, _, _ = feature_normalize(X_unscaled[:, 1:])  # Don't scale intercept\n",
    "X_scaled = np.column_stack([np.ones(X_scaled.shape[0]), X_scaled])\n",
    "theta_scaled = np.zeros((X_scaled.shape[1], 1))\n",
    "theta_scaled_opt, _ = gradient_descent(X_scaled, y_test, theta_scaled, 0.01, 1000)\n",
    "\n",
    "print(f\"Without scaling - theta: {theta_unscaled_opt.flatten()}\")\n",
    "print(f\"With scaling - theta: {theta_scaled_opt.flatten()}\")\n",
    "print(\"Feature scaling helps gradient descent converge faster and more reliably!\")\n",
    "\n",
    "# Exercise 9.3 Solution: Convergence Analysis\n",
    "print(\"\\n=== Exercise 9.3: Convergence Analysis ===\")\n",
    "theta_convergence = np.zeros((X.shape[1], 1))\n",
    "theta_conv_opt, cost_history_conv = gradient_descent(X, y, theta_convergence, 0.01, 2000)\n",
    "\n",
    "# Find when cost stops decreasing significantly\n",
    "cost_diffs = np.abs(np.diff(cost_history_conv))\n",
    "convergence_threshold = 1e-6\n",
    "convergence_iter = np.where(cost_diffs < convergence_threshold)[0]\n",
    "\n",
    "if len(convergence_iter) > 0:\n",
    "    print(f\"Algorithm converged after {convergence_iter[0]} iterations\")\n",
    "else:\n",
    "    print(\"Algorithm didn't fully converge within 2000 iterations\")\n",
    "    \n",
    "print(f\"Final cost: {cost_history_conv[-1]:.6f}\")\n",
    "print(\"Running for more iterations won't significantly improve the solution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "1. **Linear Regression** is a fundamental supervised learning algorithm for predicting continuous values.\n",
    "\n",
    "2. **Cost Function** (MSE) measures how well our model fits the data - we want to minimize this.\n",
    "\n",
    "3. **Gradient Descent** is an optimization algorithm that iteratively updates parameters to minimize the cost.\n",
    "\n",
    "4. **Learning Rate** (α) controls how big each step is - too small is slow, too large may not converge.\n",
    "\n",
    "5. **Feature Scaling** helps gradient descent converge faster and more reliably.\n",
    "\n",
    "6. **Vectorization** makes our code efficient and is crucial for large datasets.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll extend these concepts to multiple linear regression with multiple features and explore more advanced topics like feature engineering and polynomial regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
