{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Specialization - Unsupervised Learning\n",
    "## Week 3: Reinforcement Learning with Deep Q-Network\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand reinforcement learning fundamentals and key concepts\n",
    "- Learn about Markov Decision Processes and value functions\n",
    "- Implement Q-learning algorithm from scratch\n",
    "- Build and train a Deep Q-Network (DQN) using TensorFlow\n",
    "- Apply RL to solve classic control problems\n",
    "\n",
    "### Key Concepts:\n",
    "- **Reinforcement Learning**: Learning through interaction with environments\n",
    "- **Markov Decision Process**: Mathematical framework for sequential decision making\n",
    "- **Q-Learning**: Model-free algorithm for learning optimal action values\n",
    "- **Deep Q-Network**: Neural network approximation of Q-function\n",
    "- **Experience Replay**: Technique to stabilize and improve learning\n",
    "\n",
    "Reinforcement learning enables agents to learn optimal behaviors through trial-and-error interaction with environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Required Libraries\n",
    "\n",
    "Let's import the necessary libraries for our reinforcement learning exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import gym\n",
    "from gym.wrappers import RecordVideo\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Gym version: {gym.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Reinforcement Learning Fundamentals\n",
    "\n",
    "Reinforcement learning involves an agent interacting with an environment to maximize cumulative rewards:\n",
    "\n",
    "- **Agent**: Decision-making entity\n",
    "- **Environment**: External system the agent interacts with\n",
    "- **State**: Current situation of the environment\n",
    "- **Action**: Choices available to the agent\n",
    "- **Reward**: Feedback signal indicating desirability of actions\n",
    "- **Policy**: Strategy for choosing actions given states\n",
    "- **Value Function**: Expected cumulative reward from states/actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate a simple Grid World environment\n",
    "class SimpleGridWorld:\n",
    "    \"\"\"A simple 4x4 grid world for RL demonstration\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.grid_size = 4\n",
    "        self.n_states = self.grid_size * self.grid_size\n",
    "        self.n_actions = 4  # Up, Down, Left, Right\n",
    "        \n",
    "        # Define rewards\n",
    "        self.rewards = np.full((self.grid_size, self.grid_size), -0.1)  # Small negative reward for each step\n",
    "        self.rewards[0, 3] = 1.0   # Goal state\n",
    "        self.rewards[1, 1] = -1.0  # Penalty state\n",
    "        self.rewards[1, 2] = -1.0  # Penalty state\n",
    "        \n",
    "        # Reset to start position\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to initial state\"\"\"\n",
    "        self.position = (3, 0)  # Bottom-left corner\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"Convert position to state index\"\"\"\n",
    "        return self.position[0] * self.grid_size + self.position[1]\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action and return next state, reward, done\"\"\"\n",
    "        row, col = self.position\n",
    "        \n",
    "        # Action mapping: 0=Up, 1=Down, 2=Left, 3=Right\n",
    "        if action == 0:   # Up\n",
    "            row = max(0, row - 1)\n",
    "        elif action == 1: # Down\n",
    "            row = min(self.grid_size - 1, row + 1)\n",
    "        elif action == 2: # Left\n",
    "            col = max(0, col - 1)\n",
    "        elif action == 3: # Right\n",
    "            col = min(self.grid_size - 1, col + 1)\n",
    "        \n",
    "        self.position = (row, col)\n",
    "        \n",
    "        # Check if episode is done\n",
    "        done = (row == 0 and col == 3)  # Reached goal\n",
    "        \n",
    "        reward = self.rewards[row, col]\n",
    "        \n",
    "        return self._get_state(), reward, done, {}\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Render the current state\"\"\"\n",
    "        grid = np.full((self.grid_size, self.grid_size), '.')\n",
    "        grid[self.rewards == 1.0] = 'G'  # Goal\n",
    "        grid[self.rewards == -1.0] = 'P'  # Penalty\n",
    "        \n",
    "        row, col = self.position\n",
    "        grid[row, col] = 'A'  # Agent\n",
    "        \n",
    "        print(\"Grid World:\")\n",
    "        for r in range(self.grid_size):\n",
    "            print(' '.join(grid[r]))\n",
    "        print()\n",
    "\n",
    "# Test the environment\n",
    "env = SimpleGridWorld()\n",
    "print(\"Initial state:\")\n",
    "env.render()\n",
    "\n",
    "# Take some actions\n",
    "actions = [3, 3, 0, 0, 3]  # Right, Right, Up, Up, Right\n",
    "action_names = ['Right', 'Right', 'Up', 'Up', 'Right']\n",
    "\n",
    "state = env.reset()\n",
    "total_reward = 0\n",
    "\n",
    "print(\"Taking actions:\")\n",
    "for i, (action, name) in enumerate(zip(actions, action_names)):\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    print(f\"Step {i+1}: {name} -> State {next_state}, Reward {reward:.1f}, Total Reward {total_reward:.1f}\")\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(\"Episode finished! Goal reached.\")\n",
    "        break\n",
    "\n",
    "# Visualize the reward structure\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(env.rewards, cmap='RdYlGn', origin='upper')\n",
    "plt.colorbar(label='Reward')\n",
    "plt.title('Grid World Reward Structure')\n",
    "plt.xlabel('Column')\n",
    "plt.ylabel('Row')\n",
    "plt.xticks(range(4))\n",
    "plt.yticks(range(4))\n",
    "\n",
    "# Mark special states\n",
    "plt.text(3, 0, 'Start', ha='center', va='center', fontsize=12, weight='bold')\n",
    "plt.text(3, 3, 'Goal\\n(+1)', ha='center', va='center', fontsize=12, weight='bold')\n",
    "plt.text(1, 1, 'Penalty\\n(-1)', ha='center', va='center', fontsize=12, weight='bold')\n",
    "plt.text(2, 1, 'Penalty\\n(-1)', ha='center', va='center', fontsize=12, weight='bold')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Q-Learning Algorithm\n",
    "\n",
    "Q-learning is a model-free reinforcement learning algorithm that learns the optimal action-value function Q(s,a):\n",
    "\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$$\n",
    "\n",
    "Where:\n",
    "- Î± is the learning rate\n",
    "- Î³ is the discount factor\n",
    "- r is the immediate reward\n",
    "- s' is the next state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, n_episodes=1000, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Implement Q-learning algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: Environment with reset() and step() methods\n",
    "        n_episodes: Number of training episodes\n",
    "        alpha: Learning rate\n",
    "        gamma: Discount factor\n",
    "        epsilon: Exploration rate\n",
    "    \n",
    "    Returns:\n",
    "        Q: Learned Q-table\n",
    "        episode_rewards: Rewards per episode\n",
    "    \"\"\"\n",
    "    n_states = env.n_states\n",
    "    n_actions = env.n_actions\n",
    "    \n",
    "    # Initialize Q-table\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Epsilon-greedy action selection\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.randint(n_actions)  # Explore\n",
    "            else:\n",
    "                action = np.argmax(Q[state])  # Exploit\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Q-learning update\n",
    "            best_next_action = np.argmax(Q[next_state])\n",
    "            td_target = reward + gamma * Q[next_state, best_next_action]\n",
    "            td_error = td_target - Q[state, action]\n",
    "            Q[state, action] += alpha * td_error\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}: Total Reward = {total_reward:.2f}\")\n",
    "    \n",
    "    return Q, episode_rewards\n",
    "\n",
    "# Train Q-learning agent\n",
    "env_ql = SimpleGridWorld()\n",
    "Q_table, rewards_history = q_learning(env_ql, n_episodes=1000, alpha=0.1, gamma=0.9, epsilon=0.1)\n",
    "\n",
    "print(f\"\\nQ-learning completed!\")\n",
    "print(f\"Final Q-table shape: {Q_table.shape}\")\n",
    "\n",
    "# Visualize learning progress\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards_history)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Q-Learning Training Progress')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Moving average of rewards\n",
    "window_size = 50\n",
    "moving_avg = np.convolve(rewards_history, np.ones(window_size)/window_size, mode='valid')\n",
    "plt.plot(range(window_size-1, len(rewards_history)), moving_avg, 'r-', linewidth=2, label='Moving Average')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Visualize learned Q-values\n",
    "q_grid = Q_table.reshape(4, 4, 4)  # 4x4 grid, 4 actions\n",
    "best_actions = np.argmax(q_grid, axis=2)\n",
    "\n",
    "plt.imshow(best_actions, cmap='viridis')\n",
    "plt.colorbar(label='Best Action (0=Up, 1=Down, 2=Left, 3=Right)')\n",
    "plt.title('Learned Policy (Best Actions)')\n",
    "plt.xlabel('Column')\n",
    "plt.ylabel('Row')\n",
    "plt.xticks(range(4))\n",
    "plt.yticks(range(4))\n",
    "\n",
    "# Add action arrows\n",
    "action_arrows = ['â†‘', 'â†“', 'â†', 'â†’']\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        plt.text(j, i, action_arrows[best_actions[i, j]], \n",
    "                ha='center', va='center', fontsize=12, weight='bold')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test learned policy\n",
    "def test_policy(env, Q, n_episodes=10):\n",
    "    \"\"\"Test the learned policy\"\"\"\n",
    "    total_rewards = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while not done and steps < 50:  # Prevent infinite loops\n",
    "            action = np.argmax(Q[state])\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "        print(f\"Test Episode {episode + 1}: Reward = {episode_reward:.2f}, Steps = {steps}\")\n",
    "    \n",
    "    return total_rewards\n",
    "\n",
    "print(\"\\nTesting learned policy:\")\n",
    "test_rewards = test_policy(SimpleGridWorld(), Q_table)\n",
    "print(f\"Average test reward: {np.mean(test_rewards):.2f} (+/- {np.std(test_rewards):.2f})\")\n",
    "\n",
    "# Show Q-values for a specific state\n",
    "goal_state = 3  # Top-right corner (goal)\n",
    "print(f\"\\nQ-values for goal state {goal_state}: {Q_table[goal_state]}\")\n",
    "print(f\"Best action: {np.argmax(Q_table[goal_state])} ({['Up', 'Down', 'Left', 'Right'][np.argmax(Q_table[goal_state])]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Deep Q-Network (DQN)\n",
    "\n",
    "Deep Q-Networks use neural networks to approximate the Q-function, enabling RL in high-dimensional state spaces:\n",
    "\n",
    "Key innovations:\n",
    "- **Experience Replay**: Store and sample past experiences\n",
    "- **Target Network**: Stabilize learning with separate target Q-network\n",
    "- **Îµ-greedy Exploration**: Balance exploration and exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. CartPole Environment\n",
    "\n",
    "Let's apply DQN to the classic CartPole environment from OpenAI Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CartPole environment\n",
    "env_cartpole = gym.make('CartPole-v1')\n",
    "print(f\"CartPole environment:\")\n",
    "print(f\"Observation space: {env_cartpole.observation_space}\")\n",
    "print(f\"Action space: {env_cartpole.action_space}\")\n",
    "print(f\"Observation space shape: {env_cartpole.observation_space.shape}\")\n",
    "print(f\"Number of actions: {env_cartpole.action_space.n}\")\n",
    "\n",
    "# Test random policy\n",
    "def test_random_policy(env, n_episodes=5):\n",
    "    \"\"\"Test random policy on environment\"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while not done and steps < 200:\n",
    "            action = env.action_space.sample()  # Random action\n",
    "            state, reward, done, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        rewards.append(episode_reward)\n",
    "        print(f\"Random Episode {episode + 1}: {episode_reward:.0f} steps\")\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "print(\"\\nTesting random policy:\")\n",
    "random_rewards = test_random_policy(env_cartpole)\n",
    "print(f\"Average random performance: {np.mean(random_rewards):.1f} (+/- {np.std(random_rewards):.1f}) steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Implementing Deep Q-Network\n",
    "\n",
    "Let's build a DQN agent with experience replay and target network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"Deep Q-Network Agent\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, learning_rate=0.001, gamma=0.99,\n",
    "                 epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995,\n",
    "                 buffer_size=10000, batch_size=64):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Experience replay buffer\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        \n",
    "        # Main Q-network\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "        # Target Q-network\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_network()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \"\"\"Build neural network model\"\"\"\n",
    "        model = Sequential([\n",
    "            Dense(24, activation='relu', input_shape=(self.state_dim,)),\n",
    "            Dense(24, activation='relu'),\n",
    "            Dense(self.action_dim, activation='linear')\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer=Adam(learning_rate=self.learning_rate), loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update target network weights\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay buffer\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"\"\"Choose action using Îµ-greedy policy\"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.randint(self.action_dim)  # Explore\n",
    "        \n",
    "        q_values = self.model.predict(state.reshape(1, -1), verbose=0)\n",
    "        return np.argmax(q_values[0])  # Exploit\n",
    "    \n",
    "    def replay(self):\n",
    "        \"\"\"Train on batch of experiences\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample batch\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, targets = [], []\n",
    "        \n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            target = self.model.predict(state.reshape(1, -1), verbose=0)[0]\n",
    "            \n",
    "            if done:\n",
    "                target[action] = reward\n",
    "            else:\n",
    "                # Use target network for stable learning\n",
    "                next_q_values = self.target_model.predict(next_state.reshape(1, -1), verbose=0)[0]\n",
    "                target[action] = reward + self.gamma * np.max(next_q_values)\n",
    "            \n",
    "            states.append(state)\n",
    "            targets.append(target)\n",
    "        \n",
    "        # Train on batch\n",
    "        self.model.fit(np.array(states), np.array(targets), epochs=1, verbose=0, batch_size=self.batch_size)\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def load(self, name):\n",
    "        \"\"\"Load model weights\"\"\"\n",
    "        self.model.load_weights(name)\n",
    "    \n",
    "    def save(self, name):\n",
    "        \"\"\"Save model weights\"\"\"\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "# Create DQN agent\n",
    "state_dim = env_cartpole.observation_space.shape[0]\n",
    "action_dim = env_cartpole.action_space.n\n",
    "\n",
    "agent = DQNAgent(state_dim=state_dim, action_dim=action_dim)\n",
    "print(f\"DQN Agent created:\")\n",
    "print(f\"State dimension: {state_dim}\")\n",
    "print(f\"Action dimension: {action_dim}\")\n",
    "agent.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Training the DQN Agent\n",
    "\n",
    "Let's train our DQN agent on the CartPole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_episodes = 200\n",
    "max_steps = 500\n",
    "target_update_freq = 10  # Update target network every N episodes\n",
    "\n",
    "# Training loop\n",
    "episode_rewards = []\n",
    "epsilon_history = []\n",
    "\n",
    "print(\"Starting DQN training...\")\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    state = env_cartpole.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "    \n",
    "    while not done and step < max_steps:\n",
    "        # Choose action\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        # Take action\n",
    "        next_state, reward, done, _ = env_cartpole.step(action)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # Store experience\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Train on experience\n",
    "        agent.replay()\n",
    "        \n",
    "        state = next_state\n",
    "        step += 1\n",
    "    \n",
    "    episode_rewards.append(episode_reward)\n",
    "    epsilon_history.append(agent.epsilon)\n",
    "    \n",
    "    # Update target network\n",
    "    if episode % target_update_freq == 0:\n",
    "        agent.update_target_network()\n",
    "    \n",
    "    if episode % 20 == 0:\n",
    "        print(f\"Episode {episode}: Reward = {episode_reward:.1f}, Epsilon = {agent.epsilon:.3f}\")\n",
    "\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Final epsilon: {agent.epsilon:.3f}\")\n",
    "print(f\"Max reward achieved: {max(episode_rewards):.1f}\")\n",
    "print(f\"Average reward (last 50 episodes): {np.mean(episode_rewards[-50:]):.1f}\")\n",
    "\n",
    "# Visualize training progress\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode Reward')\n",
    "plt.title('DQN Training Progress')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Moving average\n",
    "window_size = 20\n",
    "if len(episode_rewards) >= window_size:\n",
    "    moving_avg = np.convolve(episode_rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    plt.plot(range(window_size-1, len(episode_rewards)), moving_avg, 'r-', linewidth=2, label='Moving Average')\n",
    "    plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epsilon_history)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.title('Exploration Rate Decay')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "# Test current policy\n",
    "test_rewards = []\n",
    "for _ in range(10):\n",
    "    state = env_cartpole.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "    \n",
    "    while not done and step < 500:\n",
    "        action = agent.act(state)  # Use current epsilon\n",
    "        state, reward, done, _ = env_cartpole.step(action)\n",
    "        episode_reward += reward\n",
    "        step += 1\n",
    "    \n",
    "    test_rewards.append(episode_reward)\n",
    "\n",
    "plt.hist(test_rewards, bins=20, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(np.mean(test_rewards), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(test_rewards):.1f}')\n",
    "plt.xlabel('Episode Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Test Performance Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test with greedy policy (no exploration)\n",
    "agent.epsilon = 0  # Disable exploration\n",
    "greedy_rewards = []\n",
    "\n",
    "for _ in range(10):\n",
    "    state = env_cartpole.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "    \n",
    "    while not done and step < 500:\n",
    "        action = agent.act(state)\n",
    "        state, reward, done, _ = env_cartpole.step(action)\n",
    "        episode_reward += reward\n",
    "        step += 1\n",
    "    \n",
    "    greedy_rewards.append(episode_reward)\n",
    "\n",
    "print(f\"\\nGreedy policy performance:\")\n",
    "print(f\"Average reward: {np.mean(greedy_rewards):.1f} (+/- {np.std(greedy_rewards):.1f})\")\n",
    "print(f\"Max reward: {max(greedy_rewards):.1f}\")\n",
    "print(f\"Success rate (reward >= 195): {np.mean(np.array(greedy_rewards) >= 195) * 100:.1f}%\")\n",
    "\n",
    "# CartPole is solved when average reward >= 195 over 100 consecutive episodes\n",
    "if np.mean(greedy_rewards) >= 195:\n",
    "    print(\"ðŸŽ‰ CartPole SOLVED! ðŸŽ‰\")\n",
    "else:\n",
    "    print(\"Keep training to solve CartPole!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},\n",
   "source": [
    "### 8. Analyzing Learned Behavior\n",
    "\n",
    "Let's analyze what our DQN agent has learned by examining its Q-values and decision patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Q-values for different states\n",
    "def analyze_q_values(agent, n_samples=1000):\n",
    "    \"\"\"Analyze Q-values across different states\"\"\"\n",
    "    q_values = []\n",
    "    states = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        state = env_cartpole.reset()\n",
    "        q_val = agent.model.predict(state.reshape(1, -1), verbose=0)[0]\n",
    "        q_values.append(q_val)\n",
    "        states.append(state)\n",
    "    \n",
    "    q_values = np.array(q_values)\n",
    "    states = np.array(states)\n",
    "    \n",
    "    return states, q_values\n",
    "\n",
    "# Analyze learned Q-values\n",
    "states, q_values = analyze_q_values(agent)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot Q-value distributions\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.hist(q_values[:, 0], alpha=0.7, label='Action 0 (Left)', bins=30)\n",
    "plt.hist(q_values[:, 1], alpha=0.7, label='Action 1 (Right)', bins=30)\n",
    "plt.xlabel('Q-Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Q-Value Distributions')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot Q-value differences\n",
    "plt.subplot(2, 3, 2)\n",
    "q_diff = q_values[:, 1] - q_values[:, 0]  # Right - Left preference\n",
    "plt.hist(q_diff, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "plt.axvline(np.mean(q_diff), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(q_diff):.2f}')\n",
    "plt.xlabel('Q(Right) - Q(Left)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Preference Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot Q-values vs state features\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.scatter(states[:, 0], q_values[:, 0], alpha=0.6, label='Q(Left)', s=10)\n",
    "plt.scatter(states[:, 0], q_values[:, 1], alpha=0.6, label='Q(Right)', s=10)\n",
    "plt.xlabel('Cart Position')\n",
    "plt.ylabel('Q-Value')\n",
    "plt.title('Q-Values vs Cart Position')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.scatter(states[:, 1], q_values[:, 0], alpha=0.6, label='Q(Left)', s=10)\n",
    "plt.scatter(states[:, 1], q_values[:, 1], alpha=0.6, label='Q(Right)', s=10)\n",
    "plt.xlabel('Cart Velocity')\n",
    "plt.ylabel('Q-Value')\n",
    "plt.title('Q-Values vs Cart Velocity')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.scatter(states[:, 2], q_values[:, 0], alpha=0.6, label='Q(Left)', s=10)\n",
    "plt.scatter(states[:, 2], q_values[:, 1], alpha=0.6, label='Q(Right)', s=10)\n",
    "plt.xlabel('Pole Angle')\n",
    "plt.ylabel('Q-Value')\n",
    "plt.title('Q-Values vs Pole Angle')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.scatter(states[:, 3], q_values[:, 0], alpha=0.6, label='Q(Left)', s=10)\n",
    "plt.scatter(states[:, 3], q_values[:, 1], alpha=0.6, label='Q(Right)', s=10)\n",
    "plt.xlabel('Pole Velocity')\n",
    "plt.ylabel('Q-Value')\n",
    "plt.title('Q-Values vs Pole Velocity')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze decision boundaries\n",
    "print(\"\\nAnalyzing learned policy:\")\n",
    "preferred_actions = np.argmax(q_values, axis=1)\n",
    "action_counts = np.bincount(preferred_actions)\n",
    "print(f\"Preferred actions distribution: Left={action_counts[0]}, Right={action_counts[1]}\")\n",
    "print(f\"Action preference ratio: {action_counts[1] / action_counts[0]:.2f} (Right:Left)\")\n",
    "\n",
    "# High-confidence decisions\n",
    "q_max = np.max(q_values, axis=1)\n",
    "q_diff = np.abs(q_values[:, 0] - q_values[:, 1])\n",
    "high_confidence = q_diff > np.percentile(q_diff, 75)\n",
    "print(f\"High confidence decisions: {np.sum(high_confidence)} / {len(high_confidence)} ({np.sum(high_confidence)/len(high_confidence)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Advanced DQN Techniques\n",
    "\n",
    "Let's explore some advanced techniques that improve DQN performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double DQN implementation\n",
    "class DoubleDQNAgent(DQNAgent):\n",
    "    \"\"\"Double DQN Agent with reduced overestimation bias\"\"\"\n",
    "    \n",
    "    def replay(self):\n",
    "        \"\"\"Double DQN training with reduced overestimation\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, targets = [], []\n",
    "        \n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            target = self.model.predict(state.reshape(1, -1), verbose=0)[0]\n",
    "            \n",
    "            if done:\n",
    "                target[action] = reward\n",
    "            else:\n",
    "                # Double DQN: Use main network to select action, target network to evaluate\n",
    "                main_q_next = self.model.predict(next_state.reshape(1, -1), verbose=0)[0]\n",
    "                best_action = np.argmax(main_q_next)\n",
    "                target_q_next = self.target_model.predict(next_state.reshape(1, -1), verbose=0)[0]\n",
    "                target[action] = reward + self.gamma * target_q_next[best_action]\n",
    "            \n",
    "            states.append(state)\n",
    "            targets.append(target)\n",
    "        \n",
    "        self.model.fit(np.array(states), np.array(targets), epochs=1, verbose=0, batch_size=self.batch_size)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Dueling DQN architecture\n",
    "def build_dueling_dqn(state_dim, action_dim):\n",
    "    \"\"\"Build Dueling DQN architecture\"\"\"\n",
    "    state_input = Input(shape=(state_dim,))\n",
    "    \n",
    "    # Shared feature layers\n",
    "    shared = Dense(32, activation='relu')(state_input)\n",
    "    shared = Dense(32, activation='relu')(shared)\n",
    "    \n",
    "    # Value stream (estimates state value)\n",
    "    value_stream = Dense(16, activation='relu')(shared)\n",
    "    value = Dense(1)(value_stream)  # V(s)\n",
    "    \n",
    "    # Advantage stream (estimates advantage of each action)\n",
    "    advantage_stream = Dense(16, activation='relu')(shared)\n",
    "    advantage = Dense(action_dim)(advantage_stream)  # A(s,a)\n",
    "    \n",
    "    # Combine: Q(s,a) = V(s) + A(s,a) - mean(A(s,a))\n",
    "    advantage_mean = tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=1, keepdims=True))(advantage)\n",
    "    q_values = tf.keras.layers.Add()([value, tf.keras.layers.Subtract()([advantage, advantage_mean])])\n",
    "    \n",
    "    model = Model(inputs=state_input, outputs=q_values)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Compare different DQN variants\n",
    "def train_dqn_variant(agent_class, env, episodes=100, variant_name=\"DQN\"):\n",
    "    \"\"\"Train a DQN variant and return performance\"\"\"\n",
    "    if variant_name == \"Dueling DQN\":\n",
    "        agent = DQNAgent(state_dim=state_dim, action_dim=action_dim)\n",
    "        agent.model = build_dueling_dqn(state_dim, action_dim)\n",
    "        agent.target_model = build_dueling_dqn(state_dim, action_dim)\n",
    "    elif variant_name == \"Double DQN\":\n",
    "        agent = DoubleDQNAgent(state_dim=state_dim, action_dim=action_dim)\n",
    "    else:\n",
    "        agent = agent_class(state_dim=state_dim, action_dim=action_dim)\n",
    "    \n",
    "    rewards = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        step = 0\n",
    "        \n",
    "        while not done and step < 200:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            agent.replay()\n",
    "            \n",
    "            state = next_state\n",
    "            step += 1\n",
    "        \n",
    "        rewards.append(episode_reward)\n",
    "        \n",
    "        if episode % 25 == 0:\n",
    "            agent.update_target_network()\n",
    "    \n",
    "    return agent, rewards\n",
    "\n",
    "# Train different variants (reduced episodes for time)\n",
    "print(\"Comparing DQN variants...\")\n",
    "variants = ['Standard DQN', 'Double DQN']\n",
    "variant_results = {}\n",
    "\n",
    "for variant in variants:\n",
    "    print(f\"\\nTraining {variant}...\")\n",
    "    agent_var, rewards_var = train_dqn_variant(DQNAgent, env_cartpole, episodes=50, variant_name=variant)\n",
    "    variant_results[variant] = {\n",
    "        'final_avg_reward': np.mean(rewards_var[-10:]),\n",
    "        'max_reward': max(rewards_var),\n",
    "        'rewards': rewards_var\n",
    "    }\n",
    "    print(f\"{variant}: Final avg reward = {np.mean(rewards_var[-10:]):.1f}\")\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for variant in variants:\n",
    "    plt.plot(variant_results[variant]['rewards'], label=variant)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode Reward')\n",
    "plt.title('DQN Variants Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "variant_names = list(variant_results.keys())\n",
    "final_rewards = [variant_results[v]['final_avg_reward'] for v in variant_names]\n",
    "plt.bar(variant_names, final_rewards)\n",
    "plt.ylabel('Final Average Reward')\n",
    "plt.title('Final Performance Comparison')\n",
    "for i, reward in enumerate(final_rewards):\n",
    "    plt.text(i, reward + 0.5, f'{reward:.1f}', ha='center')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDQN Variants Summary:\")\n",
    "for variant in variants:\n",
    "    result = variant_results[variant]\n",
    "    print(f\"{variant}:\")\n",
    "    print(f\"  Max reward: {result['max_reward']:.1f}\")\n",
    "    print(f\"  Final avg: {result['final_avg_reward']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Experimentation and Questions\n",
    "\n",
    "Now let's explore some advanced reinforcement learning concepts:\n",
    "\n",
    "1. **Hyperparameter Sensitivity**: How do learning rate, gamma, and batch size affect DQN performance?\n",
    "\n",
    "2. **Network Architecture**: Experiment with deeper/wider networks and different activation functions.\n",
    "\n",
    "3. **Exploration Strategies**: Try different exploration methods beyond Îµ-greedy.\n",
    "\n",
    "4. **Reward Shaping**: Modify the reward function and observe the impact on learning.\n",
    "\n",
    "5. **Transfer Learning**: Can knowledge from one environment transfer to another?\n",
    "\n",
    "**Challenge**: Implement a policy gradient method (like REINFORCE) and compare it with DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Hyperparameter sensitivity\n",
    "def experiment_hyperparameters():\n",
    "    \"\"\"Experiment with different DQN hyperparameters\"\"\"\n",
    "    \n",
    "    configs = [\n",
    "        {'learning_rate': 0.001, 'gamma': 0.99, 'batch_size': 64, 'name': 'Default'},\n",
    "        {'learning_rate': 0.0001, 'gamma': 0.99, 'batch_size': 64, 'name': 'Low LR'},\n",
    "        {'learning_rate': 0.01, 'gamma': 0.99, 'batch_size': 64, 'name': 'High LR'},\n",
    "        {'learning_rate': 0.001, 'gamma': 0.9, 'batch_size': 64, 'name': 'Low Gamma'},\n",
    "        {'learning_rate': 0.001, 'gamma': 0.999, 'batch_size': 64, 'name': 'High Gamma'},\n",
    "        {'learning_rate': 0.001, 'gamma': 0.99, 'batch_size': 32, 'name': 'Small Batch'},\n",
    "        {'learning_rate': 0.001, 'gamma': 0.99, 'batch_size': 128, 'name': 'Large Batch'}\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for config in configs:\n",
    "        print(f\"Testing {config['name']}...\")\n",
    "        \n",
    "        agent = DQNAgent(\n",
    "            state_dim=state_dim, \n",
    "            action_dim=action_dim,\n",
    "            learning_rate=config['learning_rate'],\n",
    "            gamma=config['gamma'],\n",
    "            batch_size=config['batch_size'],\n",
    "            epsilon=0.5,  # Start with lower exploration\n",
    "            epsilon_min=0.01\n",
    "        )\n",
    "        \n",
    "        # Quick training (reduced episodes)\n",
    "        episode_rewards = []\n",
    "        for episode in range(30):\n",
    "            state = env_cartpole.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            step = 0\n",
    "            \n",
    "            while not done and step < 200:\n",
    "                action = agent.act(state)\n",
    "                next_state, reward, done, _ = env_cartpole.step(action)\n",
    "                episode_reward += reward\n",
    "                \n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "                agent.replay()\n",
    "                \n",
    "                state = next_state\n",
    "                step += 1\n",
    "            \n",
    "            episode_rewards.append(episode_reward)\n",
    "            \n",
    "            if episode % 10 == 0:\n",
    "                agent.update_target_network()\n",
    "        \n",
    "        final_avg = np.mean(episode_rewards[-10:])\n",
    "        results.append({\n",
    "            'config': config,\n",
    "            'final_avg_reward': final_avg,\n",
    "            'max_reward': max(episode_rewards)\n",
    "        })\n",
    "        \n",
    "        print(f\"{config['name']}: Final avg = {final_avg:.1f}, Max = {max(episode_rewards):.1f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run hyperparameter experiments\n",
    "hp_results = experiment_hyperparameters()\n",
    "\n",
    "# Visualize hyperparameter effects\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Learning rate effects\n",
    "lr_configs = [r for r in hp_results if 'LR' in r['config']['name'] or r['config']['name'] == 'Default']\n",
    "plt.subplot(2, 3, 1)\n",
    "lrs = [r['config']['learning_rate'] for r in lr_configs]\n",
    "rewards = [r['final_avg_reward'] for r in lr_configs]\n",
    "plt.semilogx(lrs, rewards, 'bo-', markersize=8)\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Final Average Reward')\n",
    "plt.title('Learning Rate Sensitivity')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Gamma effects\n",
    "gamma_configs = [r for r in hp_results if 'Gamma' in r['config']['name'] or r['config']['name'] == 'Default']\n",
    "plt.subplot(2, 3, 2)\n",
    "gammas = [r['config']['gamma'] for r in gamma_configs]\n",
    "rewards = [r['final_avg_reward'] for r in gamma_configs]\n",
    "plt.plot(gammas, rewards, 'go-', markersize=8)\n",
    "plt.xlabel('Gamma (Discount Factor)')\n",
    "plt.ylabel('Final Average Reward')\n",
    "plt.title('Discount Factor Sensitivity')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Batch size effects\n",
    "batch_configs = [r for r in hp_results if 'Batch' in r['config']['name'] or r['config']['name'] == 'Default']\n",
    "plt.subplot(2, 3, 3)\n",
    "batches = [r['config']['batch_size'] for r in batch_configs]\n",
    "rewards = [r['final_avg_reward'] for r in batch_configs]\n",
    "plt.plot(batches, rewards, 'ro-', markersize=8)\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Final Average Reward')\n",
    "plt.title('Batch Size Sensitivity')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Overall comparison\n",
    "plt.subplot(2, 3, 4)\n",
    "config_names = [r['config']['name'] for r in hp_results]\n",
    "final_rewards = [r['final_avg_reward'] for r in hp_results]\n",
    "plt.bar(range(len(config_names)), final_rewards)\n",
    "plt.xticks(range(len(config_names)), config_names, rotation=45)\n",
    "plt.ylabel('Final Average Reward')\n",
    "plt.title('All Configurations')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Best configuration details\n",
    "best_config = max(hp_results, key=lambda x: x['final_avg_reward'])\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.text(0.1, 0.8, f'Best Configuration:', fontsize=12, weight='bold')\n",
    "plt.text(0.1, 0.6, f\"Name: {best_config['config']['name']}\", fontsize=10)\n",
    "plt.text(0.1, 0.4, f\"LR: {best_config['config']['learning_rate']}\", fontsize=10)\n",
    "plt.text(0.1, 0.2, f\"Gamma: {best_config['config']['gamma']}\", fontsize=10)\n",
    "plt.text(0.1, 0.0, f\"Batch: {best_config['config']['batch_size']}\", fontsize=10)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(-0.1, 1)\n",
    "plt.axis('off')\n",
    "plt.title('Best Hyperparameters')\n",
    "\n",
    "# Performance distribution\n",
    "plt.subplot(2, 3, 6)\n",
    "all_rewards = [r['final_avg_reward'] for r in hp_results]\n",
    "plt.hist(all_rewards, bins=10, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(np.mean(all_rewards), color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Mean: {np.mean(all_rewards):.1f}')\n",
    "plt.xlabel('Final Average Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Performance Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nHyperparameter Experiment Summary:\")\n",
    "print(f\"Best configuration: {best_config['config']['name']}\")\n",
    "print(f\"Best performance: {best_config['final_avg_reward']:.1f}\")\n",
    "print(f\"Performance range: {min(all_rewards):.1f} - {max(all_rewards):.1f}\")\n",
    "print(f\"Average performance: {np.mean(all_rewards):.1f} (+/- {np.std(all_rewards):.1f})\")\n",
    "\n",
    "# Key insights\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"- Learning rate is critical: too low = slow learning, too high = unstable\")\n",
    "print(\"- Higher gamma values encourage long-term planning\")\n",
    "print(\"- Batch size affects training stability and speed\")\n",
    "print(\"- Hyperparameter tuning is essential for good RL performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "1. **Reinforcement Learning** enables agents to learn through interaction with environments.\n",
    "\n",
    "2. **Q-Learning** is a model-free algorithm that learns optimal action values.\n",
    "\n",
    "3. **Deep Q-Networks** use neural networks to approximate Q-functions in high-dimensional spaces.\n",
    "\n",
    "4. **Experience Replay** and **Target Networks** stabilize DQN training.\n",
    "\n",
    "5. **Exploration vs Exploitation** is a fundamental challenge solved by Îµ-greedy policies.\n",
    "\n",
    "### Congratulations!\n",
    "\n",
    "You have completed the entire Machine Learning Specialization course! ðŸŽ‰\n",
    "\n",
    "You now have hands-on experience with:\n",
    "- **Supervised Learning**: Linear/logistic regression, neural networks\n",
    "- **Advanced Algorithms**: Decision trees, ensemble methods, XGBoost\n",
    "- **Unsupervised Learning**: Clustering, anomaly detection, recommender systems\n",
    "- **Reinforcement Learning**: Q-learning, Deep Q-Networks\n",
    "\n",
    "These notebooks provide a solid foundation for further exploration in machine learning and AI. Keep experimenting and building!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
