{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Specialization - Advanced Learning Algorithms\n",
    "## Week 4: Decision Trees and Ensemble Methods\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand how decision trees work and their advantages\n",
    "- Learn ensemble methods like Random Forests and Boosting\n",
    "- Implement XGBoost for high-performance machine learning\n",
    "- Compare tree-based methods with neural networks\n",
    "- Handle both regression and classification tasks\n",
    "\n",
    "### Key Concepts:\n",
    "- **Decision Trees**: Hierarchical models that make decisions by splitting data\n",
    "- **Ensemble Methods**: Combining multiple models for better performance\n",
    "- **Random Forests**: Bagging ensemble of decision trees\n",
    "- **Boosting**: Sequential ensemble methods (AdaBoost, Gradient Boosting)\n",
    "- **XGBoost**: Optimized gradient boosting implementation\n",
    "\n",
    "Tree-based methods offer interpretability, handle mixed data types, and often perform exceptionally well without extensive preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Required Libraries\n",
    "\n",
    "Let's import the necessary libraries for our tree-based exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, make_regression, load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import seaborn as sns\n",
    "from IPython.display import Image\n",
    "import graphviz\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate Datasets for Tree-Based Methods\n",
    "\n",
    "We'll create both regression and classification datasets to demonstrate tree-based methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification dataset with non-linear patterns\n",
    "X_clf, y_clf = make_classification(\n",
    "    n_samples=1000, n_features=10, n_informative=6, n_redundant=2, \n",
    "    n_clusters_per_class=2, random_state=42\n",
    ")\n",
    "\n",
    "# Generate regression dataset\n",
    "X_reg, y_reg = make_regression(\n",
    "    n_samples=1000, n_features=8, noise=10, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Classification dataset: X = {X_clf.shape}, y = {y_clf.shape}\")\n",
    "print(f\"Regression dataset: X = {X_reg.shape}, y = {y_reg.shape}\")\n",
    "print(f\"Classification classes: {np.unique(y_clf)}\")\n",
    "\n",
    "# Split datasets\n",
    "X_clf_train, X_clf_test, y_clf_train, y_clf_test = train_test_split(\n",
    "    X_clf, y_clf, test_size=0.2, random_state=42\n",
    ")\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nClassification - Train: {X_clf_train.shape[0]}, Test: {X_clf_test.shape[0]}\")\n",
    "print(f\"Regression - Train: {X_reg_train.shape[0]}, Test: {X_reg_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Decision Trees Fundamentals\n",
    "\n",
    "Decision trees work by recursively splitting data based on feature values to minimize impurity:\n",
    "\n",
    "- **Root Node**: Starting point with all data\n",
    "- **Internal Nodes**: Decision points that split data\n",
    "- **Leaf Nodes**: Final predictions\n",
    "- **Splitting Criteria**: Gini impurity (classification) or MSE (regression)\n",
    "\n",
    "Key advantages: Interpretable, handle mixed data types, no need for feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and visualize a simple decision tree\n",
    "def build_simple_tree(max_depth=3):\n",
    "    \"\"\"Build a simple decision tree and visualize it\"\"\"\n",
    "    # Use only first 100 samples and 2 features for visualization\n",
    "    X_simple = X_clf_train[:100, :2]\n",
    "    y_simple = y_clf_train[:100]\n",
    "    \n",
    "    tree = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
    "    tree.fit(X_simple, y_simple)\n",
    "    \n",
    "    # Visualize the tree\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plot_tree(tree, feature_names=['Feature 1', 'Feature 2'], \n",
    "              class_names=['Class 0', 'Class 1'], filled=True, rounded=True)\n",
    "    plt.title(f'Decision Tree (max_depth={max_depth})')\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize decision boundaries\n",
    "    plot_decision_boundaries_tree(X_simple, y_simple, tree, f'Decision Tree (depth={max_depth})')\n",
    "    \n",
    "    return tree\n",
    "\n",
    "def plot_decision_boundaries_tree(X, y, model, title):\n",
    "    \"\"\"Plot decision boundaries for tree-based models\"\"\"\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                        np.linspace(y_min, y_max, 100))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='black', alpha=0.8)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Build trees with different depths\n",
    "trees = []\n",
    "depths = [1, 2, 3, 5, 8]\n",
    "\n",
    "for depth in depths:\n",
    "    print(f\"\\nBuilding decision tree with max_depth = {depth}\")\n",
    "    tree = build_simple_tree(depth)\n",
    "    trees.append(tree)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_acc = tree.score(X_clf_train[:100, :2], y_clf_train[:100])\n",
    "    print(f\"Training accuracy: {train_acc:.4f}\")\n",
    "\n",
    "# Compare training vs test performance for different depths\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for depth in depths:\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    tree.fit(X_clf_train, y_clf_train)\n",
    "    \n",
    "    train_acc = tree.score(X_clf_train, y_clf_train)\n",
    "    test_acc = tree.score(X_clf_test, y_clf_test)\n",
    "    \n",
    "    train_scores.append(train_acc)\n",
    "    test_scores.append(test_acc)\n",
    "    \n",
    "    print(f\"Depth {depth}: Train Acc = {train_acc:.4f}, Test Acc = {test_acc:.4f}\")\n",
    "\n",
    "# Plot bias-variance tradeoff for trees\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(depths, train_scores, 'b-o', label='Training Accuracy')\n",
    "plt.plot(depths, test_scores, 'r-o', label='Test Accuracy')\n",
    "plt.xlabel('Tree Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Bias-Variance Tradeoff: Tree Depth vs Performance')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Random Forests - Bagging Ensemble\n",
    "\n",
    "Random Forests combine multiple decision trees trained on random subsets of data:\n",
    "\n",
    "- **Bootstrap Sampling**: Random sampling with replacement\n",
    "- **Feature Randomness**: Random feature subset at each split\n",
    "- **Voting/Averaging**: Combine predictions from all trees\n",
    "\n",
    "Advantages: Reduced overfitting, better generalization, feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and evaluate Random Forest\n",
    "def evaluate_random_forest(n_estimators_list, max_depth_list):\n",
    "    \"\"\"Evaluate Random Forest with different hyperparameters\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for n_estimators in n_estimators_list:\n",
    "        for max_depth in max_depth_list:\n",
    "            rf = RandomForestClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            # Cross-validation for robust evaluation\n",
    "            cv_scores = cross_val_score(rf, X_clf_train, y_clf_train, cv=3, scoring='accuracy')\n",
    "            mean_cv_score = np.mean(cv_scores)\n",
    "            \n",
    "            results.append({\n",
    "                'n_estimators': n_estimators,\n",
    "                'max_depth': max_depth,\n",
    "                'cv_accuracy': mean_cv_score,\n",
    "                'cv_std': np.std(cv_scores)\n",
    "            })\n",
    "            \n",
    "            print(f\"RF(n_est={n_estimators}, depth={max_depth}): CV Acc = {mean_cv_score:.4f} (+/- {np.std(cv_scores):.4f})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate different Random Forest configurations\n",
    "n_estimators_list = [10, 50, 100, 200]\n",
    "max_depth_list = [3, 5, 10, None]\n",
    "\n",
    "rf_results = evaluate_random_forest(n_estimators_list, max_depth_list)\n",
    "\n",
    "# Train best Random Forest model\n",
    "best_rf_result = max(rf_results, key=lambda x: x['cv_accuracy'])\n",
    "best_rf = RandomForestClassifier(\n",
    "    n_estimators=best_rf_result['n_estimators'],\n",
    "    max_depth=best_rf_result['max_depth'],\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "best_rf.fit(X_clf_train, y_clf_train)\n",
    "\n",
    "# Final evaluation\n",
    "rf_train_acc = best_rf.score(X_clf_train, y_clf_train)\n",
    "rf_test_acc = best_rf.score(X_clf_test, y_clf_test)\n",
    "\n",
    "print(f\"\\nBest Random Forest Configuration:\")\n",
    "print(f\"n_estimators: {best_rf_result['n_estimators']}\")\n",
    "print(f\"max_depth: {best_rf_result['max_depth']}\")\n",
    "print(f\"Training Accuracy: {rf_train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {rf_test_acc:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance_rf = best_rf.feature_importances_\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(feature_importance_rf)), feature_importance_rf)\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Boosting Methods\n",
    "\n",
    "Boosting builds models sequentially, where each new model corrects errors of previous ones:\n",
    "\n",
    "- **AdaBoost**: Adapts by changing sample weights\n",
    "- **Gradient Boosting**: Fits residuals (gradients) of previous models\n",
    "- **XGBoost**: Optimized gradient boosting with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare boosting methods\n",
    "boosting_models = {\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "boosting_results = {}\n",
    "\n",
    "for name, model in boosting_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_clf_train, y_clf_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_acc = model.score(X_clf_train, y_clf_train)\n",
    "    test_acc = model.score(X_clf_test, y_clf_test)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_clf_train, y_clf_train, cv=3, scoring='accuracy')\n",
    "    \n",
    "    boosting_results[name] = {\n",
    "        'train_accuracy': train_acc,\n",
    "        'test_accuracy': test_acc,\n",
    "        'cv_mean': np.mean(cv_scores),\n",
    "        'cv_std': np.std(cv_scores),\n",
    "        'model': model\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} Results:\")\n",
    "    print(f\"  Training Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  CV Accuracy: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
    "\n",
    "# Plot boosting comparison\n",
    "model_names = list(boosting_results.keys())\n",
    "train_accs = [boosting_results[name]['train_accuracy'] for name in model_names]\n",
    "test_accs = [boosting_results[name]['test_accuracy'] for name in model_names]\n",
    "cv_means = [boosting_results[name]['cv_mean'] for name in model_names]\n",
    "cv_stds = [boosting_results[name]['cv_std'] for name in model_names]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "x = np.arange(len(model_names))\n",
    "plt.bar(x - 0.2, train_accs, 0.2, label='Training', alpha=0.8)\n",
    "plt.bar(x, test_accs, 0.2, label='Test', alpha=0.8)\n",
    "plt.bar(x + 0.2, cv_means, 0.2, label='CV Mean', alpha=0.8)\n",
    "plt.xlabel('Boosting Method')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Boosting Methods Comparison')\n",
    "plt.xticks(x, model_names, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.errorbar(model_names, cv_means, yerr=cv_stds, fmt='o-', capsize=5)\n",
    "plt.ylabel('Cross-Validation Accuracy')\n",
    "plt.title('CV Accuracy with Error Bars')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. XGBoost Deep Dive\n",
    "\n",
    "XGBoost is an optimized implementation of gradient boosting with several advantages:\n",
    "\n",
    "- **Regularization**: Built-in L1 and L2 regularization\n",
    "- **Parallel Processing**: Fast training on multi-core CPUs\n",
    "- **Tree Pruning**: Prevents overfitting through pruning\n",
    "- **Handling Missing Values**: Built-in missing value handling\n",
    "- **Early Stopping**: Automatic stopping when validation performance stops improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost hyperparameter tuning\n",
    "def tune_xgboost(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Tune XGBoost hyperparameters using validation set\"\"\"\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0],\n",
    "        'reg_alpha': [0, 0.1],  # L1 regularization\n",
    "        'reg_lambda': [1, 1.5]  # L2 regularization\n",
    "    }\n",
    "    \n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    # Simple grid search (in practice, use more sophisticated methods)\n",
    "    from itertools import product\n",
    "    \n",
    "    # Test a subset of combinations for time efficiency\n",
    "    test_params = [\n",
    "        {'max_depth': 3, 'learning_rate': 0.1, 'n_estimators': 100},\n",
    "        {'max_depth': 5, 'learning_rate': 0.1, 'n_estimators': 100},\n",
    "        {'max_depth': 3, 'learning_rate': 0.2, 'n_estimators': 100},\n",
    "        {'max_depth': 5, 'learning_rate': 0.05, 'n_estimators': 200},\n",
    "        {'max_depth': 4, 'learning_rate': 0.1, 'n_estimators': 150, 'reg_alpha': 0.1}\n",
    "    ]\n",
    "    \n",
    "    print(\"Tuning XGBoost hyperparameters...\")\n",
    "    \n",
    "    for i, params in enumerate(test_params):\n",
    "        model = XGBClassifier(\n",
    "            **params,\n",
    "            random_state=42,\n",
    "            eval_metric='logloss',\n",
    "            early_stopping_rounds=10\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "        \n",
    "        val_score = model.score(X_val, y_val)\n",
    "        \n",
    "        print(f\"Config {i+1}: {params} -> Val Acc = {val_score:.4f}\")\n",
    "        \n",
    "        if val_score > best_score:\n",
    "            best_score = val_score\n",
    "            best_params = params\n",
    "            best_model = model\n",
    "    \n",
    "    print(f\"\\nBest XGBoost configuration: {best_params}\")\n",
    "    print(f\"Best validation accuracy: {best_score:.4f}\")\n",
    "    \n",
    "    return best_model, best_params\n",
    "\n",
    "# Tune XGBoost\n",
    "X_clf_train_tune, X_clf_val_tune, y_clf_train_tune, y_clf_val_tune = train_test_split(\n",
    "    X_clf_train, y_clf_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "best_xgb, best_xgb_params = tune_xgboost(X_clf_train_tune, y_clf_train_tune, \n",
    "                                         X_clf_val_tune, y_clf_val_tune)\n",
    "\n",
    "# Final evaluation\n",
    "xgb_train_acc = best_xgb.score(X_clf_train, y_clf_train)\n",
    "xgb_test_acc = best_xgb.score(X_clf_test, y_clf_test)\n",
    "\n",
    "print(f\"\\nXGBoost Final Results:\")\n",
    "print(f\"Training Accuracy: {xgb_train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {xgb_test_acc:.4f}\")\n",
    "\n",
    "# XGBoost feature importance\n",
    "xgb.plot_importance(best_xgb, max_num_features=10)\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.show()\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(best_xgb.evals_result_['validation_0']['logloss'], label='Training Loss')\n",
    "plt.xlabel('Boosting Round')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.title('XGBoost Training Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Tree-Based Regression\n",
    "\n",
    "Tree-based methods work for regression too, using MSE as the splitting criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tree-based regression methods\n",
    "regression_models = {\n",
    "    'Decision Tree': DecisionTreeRegressor(max_depth=5, random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "regression_results = {}\n",
    "\n",
    "for name, model in regression_models.items():\n",
    "    print(f\"\\nTraining {name} for regression...\")\n",
    "    \n",
    "    model.fit(X_reg_train, y_reg_train)\n",
    "    \n",
    "    # Predictions\n",
    "    train_pred = model.predict(X_reg_train)\n",
    "    test_pred = model.predict(X_reg_test)\n",
    "    \n",
    "    # Metrics\n",
    "    train_mse = mean_squared_error(y_reg_train, train_pred)\n",
    "    test_mse = mean_squared_error(y_reg_test, test_pred)\n",
    "    train_rmse = np.sqrt(train_mse)\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    \n",
    "    regression_results[name] = {\n",
    "        'train_mse': train_mse,\n",
    "        'test_mse': test_mse,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} Results:\")\n",
    "    print(f\"  Training MSE: {train_mse:.4f}, RMSE: {train_rmse:.4f}\")\n",
    "    print(f\"  Test MSE: {test_mse:.4f}, RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "# Plot regression comparison\n",
    "model_names_reg = list(regression_results.keys())\n",
    "train_mses = [regression_results[name]['train_mse'] for name in model_names_reg]\n",
    "test_mses = [regression_results[name]['test_mse'] for name in model_names_reg]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "x_reg = np.arange(len(model_names_reg))\n",
    "plt.bar(x_reg - 0.2, train_mses, 0.4, label='Training MSE', alpha=0.8)\n",
    "plt.bar(x_reg + 0.2, test_mses, 0.4, label='Test MSE', alpha=0.8)\n",
    "plt.xlabel('Regression Method')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Tree-Based Regression Comparison')\n",
    "plt.xticks(x_reg, model_names_reg, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot predictions vs actual for best model\n",
    "best_reg_model = min(regression_results.items(), key=lambda x: x[1]['test_mse'])[0]\n",
    "best_reg = regression_models[best_reg_model]\n",
    "y_pred_best = best_reg.predict(X_reg_test)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_reg_test, y_pred_best, alpha=0.6, color='blue')\n",
    "plt.plot([y_reg_test.min(), y_reg_test.max()], [y_reg_test.min(), y_reg_test.max()], 'r--', linewidth=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title(f'{best_reg_model} Predictions vs Actual')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest regression model: {best_reg_model}\")\n",
    "print(f\"Test RMSE: {regression_results[best_reg_model]['test_rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Model Comparison: Trees vs Neural Networks\n",
    "\n",
    "Let's compare tree-based methods with neural networks on our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare best models from each category\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Neural network for classification\n",
    "nn_clf = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_clf_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "nn_clf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "nn_clf.fit(X_clf_train, y_clf_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Neural network for regression\n",
    "nn_reg = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_reg_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "nn_reg.compile(optimizer='adam', loss='mse')\n",
    "nn_reg.fit(X_reg_train, y_reg_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Evaluate neural networks\n",
    "nn_clf_acc = nn_clf.evaluate(X_clf_test, y_clf_test, verbose=0)[1]\n",
    "nn_reg_mse = nn_reg.evaluate(X_reg_test, y_reg_test, verbose=0)\n",
    "nn_reg_rmse = np.sqrt(nn_reg_mse)\n",
    "\n",
    "# Final comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nCLASSIFICATION:\")\n",
    "print(\"-\" * 30)\n",
    "models_clf = {\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    'Random Forest': best_rf,\n",
    "    'XGBoost': best_xgb,\n",
    "    'Neural Network': nn_clf\n",
    "}\n",
    "\n",
    "for name, model in models_clf.items():\n",
    "    if name == 'Neural Network':\n",
    "        acc = nn_clf_acc\n",
    "    else:\n",
    "        model.fit(X_clf_train, y_clf_train)\n",
    "        acc = model.score(X_clf_test, y_clf_test)\n",
    "    print(f\"{name:15}: Test Accuracy = {acc:.4f}\")\n",
    "\n",
    "print(\"\\nREGRESSION:\")\n",
    "print(\"-\" * 30)\n",
    "models_reg = {\n",
    "    'Decision Tree': DecisionTreeRegressor(max_depth=5, random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, random_state=42),\n",
    "    'Neural Network': nn_reg\n",
    "}\n",
    "\n",
    "for name, model in models_reg.items():\n",
    "    if name == 'Neural Network':\n",
    "        rmse = nn_reg_rmse\n",
    "    else:\n",
    "        model.fit(X_reg_train, y_reg_train)\n",
    "        pred = model.predict(X_reg_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_reg_test, pred))\n",
    "    print(f\"{name:15}: Test RMSE = {rmse:.4f}\")\n",
    "\n",
    "# Training time comparison (rough estimate)\n",
    "import time\n",
    "\n",
    "print(\"\\nTRAINING TIME COMPARISON (approximate):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Time XGBoost training\n",
    "start_time = time.time()\n",
    "xgb_model = XGBClassifier(n_estimators=100, random_state=42)\n",
    "xgb_model.fit(X_clf_train, y_clf_train)\n",
    "xgb_time = time.time() - start_time\n",
    "\n",
    "# Time Random Forest training\n",
    "start_time = time.time()\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_clf_train, y_clf_train)\n",
    "rf_time = time.time() - start_time\n",
    "\n",
    "print(f\"Random Forest: {rf_time:.2f} seconds\")\n",
    "print(f\"XGBoost:       {xgb_time:.2f} seconds\")\n",
    "print(f\"Speedup:       {rf_time/xgb_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Experimentation and Questions\n",
    "\n",
    "Now let's explore some advanced tree-based concepts:\n",
    "\n",
    "1. **Tree Pruning**: Experiment with different pruning strategies and minimum samples per leaf.\n",
    "\n",
    "2. **Feature Engineering**: How do tree-based methods handle categorical features vs neural networks?\n",
    "\n",
    "3. **Scalability**: How do different methods scale with dataset size and feature count?\n",
    "\n",
    "4. **Interpretability**: Compare the interpretability of decision trees vs complex ensembles.\n",
    "\n",
    "5. **Missing Values**: How do different methods handle missing data?\n",
    "\n",
    "**Challenge**: Implement a simple gradient boosting algorithm from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Tree hyperparameters and their effects\n",
    "def experiment_tree_params():\n",
    "    \"\"\"Experiment with different tree hyperparameters\"\"\"\n",
    "    \n",
    "    # Different configurations\n",
    "    configs = [\n",
    "        {'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 1},\n",
    "        {'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1},\n",
    "        {'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 5},\n",
    "        {'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1},  # No pruning\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, config in enumerate(configs):\n",
    "        tree = DecisionTreeClassifier(random_state=42, **config)\n",
    "        tree.fit(X_clf_train, y_clf_train)\n",
    "        \n",
    "        train_acc = tree.score(X_clf_train, y_clf_train)\n",
    "        test_acc = tree.score(X_clf_test, y_clf_test)\n",
    "        n_leaves = tree.get_n_leaves()\n",
    "        depth = tree.get_depth()\n",
    "        \n",
    "        results.append({\n",
    "            'config': config,\n",
    "            'train_acc': train_acc,\n",
    "            'test_acc': test_acc,\n",
    "            'n_leaves': n_leaves,\n",
    "            'depth': depth\n",
    "        })\n",
    "        \n",
    "        print(f\"Config {i+1}: {config}\")\n",
    "        print(f\"  Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "        print(f\"  Tree Depth: {depth}, Leaves: {n_leaves}\")\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run experiment\n",
    "param_results = experiment_tree_params()\n",
    "\n",
    "# Plot results\n",
    "configs_labels = [f\"Config {i+1}\" for i in range(len(param_results))]\n",
    "train_accs = [r['train_acc'] for r in param_results]\n",
    "test_accs = [r['test_acc'] for r in param_results]\n",
    "complexities = [r['n_leaves'] for r in param_results]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Performance vs complexity\n",
    "axes[0].scatter(complexities, train_accs, label='Training', s=100, alpha=0.7)\n",
    "axes[0].scatter(complexities, test_accs, label='Test', s=100, alpha=0.7)\n",
    "for i, label in enumerate(configs_labels):\n",
    "    axes[0].annotate(label, (complexities[i], test_accs[i]), xytext=(5, 5), textcoords='offset points')\n",
    "axes[0].set_xlabel('Number of Leaves (Complexity)')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Model Complexity vs Performance')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Bias-variance visualization\n",
    "x_pos = np.arange(len(configs_labels))\n",
    "axes[1].bar(x_pos - 0.2, train_accs, 0.4, label='Training', alpha=0.7)\n",
    "axes[1].bar(x_pos + 0.2, test_accs, 0.4, label='Test', alpha=0.7)\n",
    "axes[1].set_xlabel('Configuration')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Bias-Variance Analysis')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(configs_labels, rotation=45)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Best configuration analysis\n",
    "best_config = max(param_results, key=lambda x: x['test_acc'])\n",
    "print(f\"Best configuration: {best_config['config']}\")\n",
    "print(f\"Best test accuracy: {best_config['test_acc']:.4f}\")\n",
    "print(f\"Gap between train and test: {best_config['train_acc'] - best_config['test_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "1. **Decision Trees** are interpretable but prone to overfitting without proper constraints.\n",
    "\n",
    "2. **Random Forests** reduce overfitting through bagging and provide feature importance.\n",
    "\n",
    "3. **Boosting Methods** (especially XGBoost) often achieve state-of-the-art performance.\n",
    "\n",
    "4. **Tree-based methods** handle mixed data types, missing values, and don't require feature scaling.\n",
    "\n",
    "5. **Model Selection** depends on the problem: trees for interpretability, ensembles for performance.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the Unsupervised Learning section, we'll explore clustering and anomaly detection techniques that work without labeled data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
