{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Specialization - Supervised Learning: Regression and Classification\n",
    "## Week 1: Linear Regression with One Variable\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand the basics of linear regression\n",
    "- Implement and compute cost functions\n",
    "- Apply gradient descent algorithm\n",
    "- Build intuition through hands-on implementation\n",
    "\n",
    "### Key Concepts:\n",
    "- **Linear Regression**: A method to predict continuous values using a linear relationship\n",
    "- **Cost Function**: Measures how well our model fits the data (Mean Squared Error)\n",
    "- **Gradient Descent**: An optimization algorithm to minimize the cost function\n",
    "\n",
    "In this notebook, you'll implement linear regression from scratch using NumPy and build intuition about how these fundamental algorithms work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Required Libraries\n",
    "\n",
    "Let's start by importing the necessary libraries for our exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate Synthetic Data\n",
    "\n",
    "We'll create a simple dataset to work with. This represents a real-world scenario where we want to predict house prices based on their size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for house prices vs size\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# Add intercept term (x0 = 1) to X\n",
    "X = np.column_stack([np.ones(X.shape[0]), X])\n",
    "\n",
    "# Reshape y to be a column vector\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "print(f\"Dataset shape: X = {X.shape}, y = {y.shape}\")\n",
    "print(f\"First 5 samples:\")\n",
    "for i in range(5):\n",
    "    print(f\"House size: {X[i, 1]:.2f}, Price: {y[i, 0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualize the Data\n",
    "\n",
    "Let's plot our data to understand the relationship between house size and price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 1], y, alpha=0.7, color='blue')\n",
    "plt.xlabel('House Size (normalized)')\n",
    "plt.ylabel('House Price')\n",
    "plt.title('House Price vs Size')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Linear Regression Model\n",
    "\n",
    "The linear regression model can be expressed as:\n",
    "\n",
    "$$h_\\theta(x) = \\theta_0 + \\theta_1 x$$\n",
    "\n",
    "Or in matrix form:\n",
    "\n",
    "$$h_\\theta(X) = X \\theta$$\n",
    "\n",
    "Where:\n",
    "- $X$ is the feature matrix (with intercept column)\n",
    "- $\\theta$ is the parameter vector\n",
    "- $h_\\theta(X)$ is our prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, theta):\n",
    "    \"\"\"\n",
    "    Make predictions using linear regression model.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix (m x n)\n",
    "        theta: Parameter vector (n x 1)\n",
    "    \n",
    "    Returns:\n",
    "        predictions: Predicted values (m x 1)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE - Implement the linear regression prediction\n",
    "    predictions = None  # Replace with your implementation\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Test the function with random theta\n",
    "theta_test = np.array([[2.0], [1.5]])\n",
    "predictions_test = predict(X[:5], theta_test)\n",
    "print(f\"Test predictions: {predictions_test.flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Cost Function\n",
    "\n",
    "The cost function measures how well our model fits the data. For linear regression, we use the Mean Squared Error (MSE):\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "Where:\n",
    "- $m$ is the number of training examples\n",
    "- $h_\\theta(x^{(i)})$ is the prediction for the i-th example\n",
    "- $y^{(i)}$ is the actual value for the i-th example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the cost function for linear regression.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix (m x n)\n",
    "        y: Target values (m x 1)\n",
    "        theta: Parameter vector (n x 1)\n",
    "    \n",
    "    Returns:\n",
    "        cost: The cost value (scalar)\n",
    "    \"\"\"\n",
    "    m = len(y)  # number of training examples\n",
    "    \n",
    "    # YOUR CODE HERE - Implement the cost function\n",
    "    # Hint: Use vectorized operations for efficiency\n",
    "    cost = None  # Replace with your implementation\n",
    "    \n",
    "    return cost\n",
    "\n",
    "# Test the cost function\n",
    "theta_test = np.array([[2.0], [1.5]])\n",
    "cost_test = compute_cost(X, y, theta_test)\n",
    "print(f\"Test cost with theta = {theta_test.flatten()}: {cost_test:.4f}\")\n",
    "\n",
    "# Test with different theta values\n",
    "theta_good = np.array([[0.0], [2.0]])\n",
    "cost_good = compute_cost(X, y, theta_good)\n",
    "print(f\"Cost with better theta = {theta_good.flatten()}: {cost_good:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Gradient Descent\n",
    "\n",
    "Gradient descent is used to minimize the cost function. The algorithm updates parameters simultaneously:\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta)$$\n",
    "\n",
    "For linear regression, the partial derivative is:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ is the learning rate\n",
    "- $x_j^{(i)}$ is the j-th feature of the i-th training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, num_iterations):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to learn theta.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix (m x n)\n",
    "        y: Target values (m x 1)\n",
    "        theta: Initial parameter vector (n x 1)\n",
    "        alpha: Learning rate\n",
    "        num_iterations: Number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        theta: Optimized parameter vector\n",
    "        cost_history: List of cost values over iterations\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        # YOUR CODE HERE - Implement gradient descent update\n",
    "        # Hint: Update all theta values simultaneously\n",
    "        \n",
    "        # 1. Calculate predictions\n",
    "        predictions = None  # Replace with your implementation\n",
    "        \n",
    "        # 2. Calculate errors\n",
    "        errors = None  # Replace with your implementation\n",
    "        \n",
    "        # 3. Calculate gradient\n",
    "        gradient = None  # Replace with your implementation\n",
    "        \n",
    "        # 4. Update theta\n",
    "        theta = None  # Replace with your implementation\n",
    "        \n",
    "        # 5. Compute and store cost\n",
    "        cost = compute_cost(X, y, theta)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        # Print progress every 100 iterations\n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"Iteration {iteration}: Cost = {cost:.4f}\")\n",
    "    \n",
    "    return theta, cost_history\n",
    "\n",
    "# Initialize theta with zeros\n",
    "theta_initial = np.zeros((X.shape[1], 1))\n",
    "\n",
    "# Set hyperparameters\n",
    "alpha = 0.01\n",
    "num_iterations = 1000\n",
    "\n",
    "# Run gradient descent\n",
    "theta_optimized, cost_history = gradient_descent(X, y, theta_initial, alpha, num_iterations)\n",
    "\n",
    "print(f\"\\nOptimized theta: {theta_optimized.flatten()}\")\n",
    "print(f\"Final cost: {cost_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Visualize the Results\n",
    "\n",
    "Let's plot the cost function over iterations and the fitted line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cost vs iterations\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost Function vs Iterations')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot fitted line\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X[:, 1], y, alpha=0.7, color='blue', label='Data points')\n",
    "\n",
    "# Generate predictions for the fitted line\n",
    "X_plot = np.linspace(X[:, 1].min(), X[:, 1].max(), 100).reshape(-1, 1)\n",
    "X_plot_with_intercept = np.column_stack([np.ones(X_plot.shape[0]), X_plot])\n",
    "y_plot = predict(X_plot_with_intercept, theta_optimized)\n",
    "\n",
    "plt.plot(X_plot, y_plot, color='red', linewidth=2, label='Fitted line')\n",
    "plt.xlabel('House Size')\n",
    "plt.ylabel('House Price')\n",
    "plt.title('Linear Regression Fit')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Model Evaluation\n",
    "\n",
    "Let's evaluate our model using Mean Squared Error and visualize the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the entire dataset\n",
    "y_pred = predict(X, theta_optimized)\n",
    "\n",
    "# Calculate MSE\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "# Calculate R-squared (coefficient of determination)\n",
    "ss_res = np.sum((y - y_pred) ** 2)\n",
    "ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "r_squared = 1 - (ss_res / ss_tot)\n",
    "print(f\"R-squared: {r_squared:.4f}\")\n",
    "\n",
    "# Plot residuals\n",
    "residuals = y - y_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.7, color='green')\n",
    "plt.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Experimentation and Questions\n",
    "\n",
    "Now that you've implemented linear regression from scratch, let's explore some important concepts:\n",
    "\n",
    "1. **Learning Rate Effects**: Try different values of alpha (e.g., 0.001, 0.1, 0.5) and observe how the cost function behaves.\n",
    "\n",
    "2. **Initialization**: What happens if you initialize theta with different values instead of zeros?\n",
    "\n",
    "3. **Convergence**: How many iterations does it take for the algorithm to converge? What happens if you run it for too many iterations?\n",
    "\n",
    "4. **Feature Scaling**: What would happen if our features had very different scales? How could we address this?\n",
    "\n",
    "5. **Overfitting**: With only one feature, overfitting is unlikely. But what are some techniques to prevent overfitting in general?\n",
    "\n",
    "**Challenge**: Implement feature normalization and observe how it affects gradient descent convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment space - Try different learning rates\n",
    "alphas = [0.001, 0.01, 0.1]\n",
    "cost_histories = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    theta_temp = np.zeros((X.shape[1], 1))\n",
    "    _, cost_history = gradient_descent(X, y, theta_temp, alpha, 500)\n",
    "    cost_histories.append(cost_history)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, alpha in enumerate(alphas):\n",
    "    plt.plot(cost_histories[i], label=f'α = {alpha}')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost Function for Different Learning Rates')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "1. **Linear Regression** is a fundamental supervised learning algorithm for predicting continuous values.\n",
    "\n",
    "2. **Cost Function** (MSE) measures how well our model fits the data - we want to minimize this.\n",
    "\n",
    "3. **Gradient Descent** is an optimization algorithm that iteratively updates parameters to minimize the cost.\n",
    "\n",
    "4. **Learning Rate** (α) controls how big each step is - too small is slow, too large may not converge.\n",
    "\n",
    "5. **Vectorization** makes our code efficient and is crucial for large datasets.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll extend these concepts to multiple linear regression with multiple features and explore more advanced topics like feature engineering and polynomial regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
